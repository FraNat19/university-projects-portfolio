{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework 1 - BDA**\n",
        "### **Francesco Natali 1945581**\n",
        "\n",
        "The goal of this project is to predict whether an individual earns over $50K per year using demographic and occupational features. The task is approached as a supervised classification problem.\n"
      ],
      "metadata": {
        "id": "-6Pu77Vnbbvb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n74PUDf9Kxex",
        "outputId": "119e2a71-f850-452a-cd20-6492b3ff8224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age          workclass  fnlwgt  education-num       marital-status  \\\n",
            "0   39          State-gov   77516             13        Never-married   \n",
            "1   50   Self-emp-not-inc   83311             13   Married-civ-spouse   \n",
            "2   38            Private  215646              9             Divorced   \n",
            "3   53            Private  234721              7   Married-civ-spouse   \n",
            "4   28            Private  338409             13   Married-civ-spouse   \n",
            "\n",
            "           occupation    relationship    race      sex  capital-gain  \\\n",
            "0        Adm-clerical   Not-in-family   White     Male          2174   \n",
            "1     Exec-managerial         Husband   White     Male             0   \n",
            "2   Handlers-cleaners   Not-in-family   White     Male             0   \n",
            "3   Handlers-cleaners         Husband   Black     Male             0   \n",
            "4      Prof-specialty            Wife   Black   Female             0   \n",
            "\n",
            "   capital-loss  hours-per-week  target  \n",
            "0             0              40   <=50K  \n",
            "1             0              13   <=50K  \n",
            "2             0              40   <=50K  \n",
            "3             0              40   <=50K  \n",
            "4             0              40   <=50K  \n",
            "[' Adm-clerical' ' Exec-managerial' ' Handlers-cleaners' ' Prof-specialty'\n",
            " ' Other-service' ' Sales' ' Craft-repair' ' Transport-moving'\n",
            " ' Farming-fishing' ' Machine-op-inspct' ' Tech-support' ' ?'\n",
            " ' Protective-serv' ' Armed-Forces' ' Priv-house-serv']\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for data manipulation, preprocessing, modeling, and evaluation.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"/content/data1 (2).csv\", sep=';')\n",
        "\n",
        "# Drop education and native-country as instructed\n",
        "df = data.drop(['education', 'native-country'], axis=1)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Occupational categories are reclassified into five broader groups to reduce complexity and facilitate modeling."
      ],
      "metadata": {
        "id": "udY4ihaMcR5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine occupation into 5 categories\n",
        "# Define the mapping\n",
        "occupation_map = {\n",
        "    # Group 1: Office-based and technical roles\n",
        "    'Adm-clerical': 'Office & Tech Roles',\n",
        "    'Exec-managerial': 'Office & Tech Roles',\n",
        "    'Prof-specialty': 'Office & Tech Roles',\n",
        "    'Tech-support': 'Office & Tech Roles',\n",
        "\n",
        "    # Group 2: Sales and personal services\n",
        "    'Sales': 'Sales & Personal Services',\n",
        "    'Other-service': 'Sales & Personal Services',\n",
        "    'Priv-house-serv': 'Sales & Personal Services',\n",
        "\n",
        "    # Group 3: Technical and mechanical Work\n",
        "    'Craft-repair': 'Technical & Mechanical Work',\n",
        "    'Machine-op-inspct': 'Technical & Mechanical Work',\n",
        "    'Handlers-cleaners': 'Technical & Mechanical Work',\n",
        "    'Transport-moving': 'Technical & Mechanical Work',\n",
        "\n",
        "    # Group 4: Agriculture and military\n",
        "    'Farming-fishing': 'Agriculture & Military',\n",
        "    'Armed-Forces': 'Agriculture & Military',\n",
        "\n",
        "    # Group 5: Security and protective services\n",
        "    'Protective-serv': 'Security & Protection'\n",
        "}\n",
        "\n",
        "# First removes space\n",
        "df['occupation'] = df['occupation'].str.strip()\n",
        "# Remove rows where occupation is '?'\n",
        "df = df[df['occupation'] != '?']\n",
        "# Apply the mapping\n",
        "df['occupation_grouped'] = df['occupation'].map(occupation_map)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2APzu9iuOaLs",
        "outputId": "305bcd93-8be9-46cb-847b-69a19935f6c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age          workclass  fnlwgt  education-num       marital-status  \\\n",
            "0   39          State-gov   77516             13        Never-married   \n",
            "1   50   Self-emp-not-inc   83311             13   Married-civ-spouse   \n",
            "2   38            Private  215646              9             Divorced   \n",
            "3   53            Private  234721              7   Married-civ-spouse   \n",
            "4   28            Private  338409             13   Married-civ-spouse   \n",
            "\n",
            "          occupation    relationship    race      sex  capital-gain  \\\n",
            "0       Adm-clerical   Not-in-family   White     Male          2174   \n",
            "1    Exec-managerial         Husband   White     Male             0   \n",
            "2  Handlers-cleaners   Not-in-family   White     Male             0   \n",
            "3  Handlers-cleaners         Husband   Black     Male             0   \n",
            "4     Prof-specialty            Wife   Black   Female             0   \n",
            "\n",
            "   capital-loss  hours-per-week  target   occupation_grouped  \n",
            "0             0              40   <=50K  Office & Tech Roles  \n",
            "1             0              13   <=50K  Office & Tech Roles  \n",
            "2             0              40   <=50K        Skilled Labor  \n",
            "3             0              40   <=50K        Skilled Labor  \n",
            "4             0              40   <=50K  Office & Tech Roles  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target variable is defined and the dataset is split into training and validation subsets. Then, categorical and numerical features are identified to set up a preprocessing pipeline that standardizes numeric values and applies one-hot encoding to categorical variables."
      ],
      "metadata": {
        "id": "39q_b9XAcl0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target and features\n",
        "X = df.drop(['target'], axis=1)\n",
        "y = df['target'].str.strip()  # Rimuove spazi extra da target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=123\n",
        ")\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Remove 'education' and 'native-country' if still in the list\n",
        "categorical_cols = [col for col in categorical_cols if col not in ['education', 'native-country']]\n",
        "\n",
        "# Column transformer\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), numerical_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "])"
      ],
      "metadata": {
        "id": "6lCxR2m-Rwkm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define and evaluate three different classification models: Random Forest, Gradient Boosting, and Logistic Regression. Each model is trained using the training set and evaluated using the validation set. The performance is assessed based on accuracy, the classification report (which includes precision, recall, and F1-score), and the confusion matrix to analyze the true positives, false positives, true negatives, and false negatives."
      ],
      "metadata": {
        "id": "5P5N7PLKc8MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to evaluate\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=123),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=123),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=123)\n",
        "}\n",
        "\n",
        "# Loop over models\n",
        "for name, model in models.items():\n",
        "    clf = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_val)\n",
        "\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    print(confusion_matrix(y_val, y_pred))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bieS-ycuV8VP",
        "outputId": "5ed7401e-4f52-4c2a-fc8c-068cc0097b43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Random Forest ---\n",
            "Accuracy: 0.8516710069444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.92      0.90      6946\n",
            "        >50K       0.73      0.63      0.68      2270\n",
            "\n",
            "    accuracy                           0.85      9216\n",
            "   macro avg       0.81      0.78      0.79      9216\n",
            "weighted avg       0.85      0.85      0.85      9216\n",
            "\n",
            "[[6416  530]\n",
            " [ 837 1433]]\n",
            "\n",
            "\n",
            "--- Gradient Boosting ---\n",
            "Accuracy: 0.8648003472222222\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.95      0.91      6946\n",
            "        >50K       0.79      0.62      0.69      2270\n",
            "\n",
            "    accuracy                           0.86      9216\n",
            "   macro avg       0.84      0.78      0.80      9216\n",
            "weighted avg       0.86      0.86      0.86      9216\n",
            "\n",
            "[[6573  373]\n",
            " [ 873 1397]]\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Accuracy: 0.8489583333333334\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.93      0.90      6946\n",
            "        >50K       0.73      0.61      0.67      2270\n",
            "\n",
            "    accuracy                           0.85      9216\n",
            "   macro avg       0.81      0.77      0.78      9216\n",
            "weighted avg       0.84      0.85      0.84      9216\n",
            "\n",
            "[[6436  510]\n",
            " [ 882 1388]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**: Achieved an accuracy of 0.852, with strong precision for <=50K (0.88) but lower recall for >50K (0.63), indicating difficulty in identifying high-income individuals.\n",
        "\n",
        "**Gradient Boosting**: Slightly outperformed Random Forest with an accuracy of 0.865, similar precision for <=50K (0.88), and a slightly higher recall for >50K (0.62), but still faced challenges in predicting high-income individuals.\n",
        "\n",
        "**Logistic Regression**: Had the lowest accuracy (0.848), with strong performance for <=50K but poor precision and recall for >50K, struggling to identify high-income individuals effectively."
      ],
      "metadata": {
        "id": "-OEKNAzadYJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, In order to optimize the models and identify the best hyperparameters, we use GridSearchCV. This method performs an exhaustive search over a specified parameter grid for each model, optimizing for accuracy by evaluating different hyperparameter combinations using cross-validation. Below are the steps for hyperparameter optimization applied to the Random Forest, Gradient Boosting, and Logistic Regression models."
      ],
      "metadata": {
        "id": "7Bz6eRvFdz1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest – Grid Search\n",
        "param_grid_rf = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__max_depth': [None, 10, 20],\n",
        "    'classifier__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=123))\n",
        "])\n",
        "\n",
        "grid_rf = GridSearchCV(rf_pipeline, param_grid_rf, cv=3, n_jobs=-1, scoring='accuracy')\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best RF parameters:\", grid_rf.best_params_)\n",
        "print(\"Best RF score:\", grid_rf.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTV_8M3uX9hX",
        "outputId": "4b106e16-ecce-442d-c8a1-d0b2dab3e530"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RF parameters: {'classifier__max_depth': 20, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 200}\n",
            "Best RF score: 0.8598736660711794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting – Grid Search\n",
        "param_grid_gb = {\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__learning_rate': [0.1, 0.05],\n",
        "    'classifier__max_depth': [3, 5]\n",
        "}\n",
        "\n",
        "gb_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', GradientBoostingClassifier(random_state=123))\n",
        "])\n",
        "\n",
        "grid_gb = GridSearchCV(gb_pipeline, param_grid_gb, cv=3, n_jobs=-1, scoring='accuracy')\n",
        "grid_gb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best GB parameters:\", grid_gb.best_params_)\n",
        "print(\"Best GB score:\", grid_gb.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8mpwB7rYCxX",
        "outputId": "e32760be-db4f-4777-e59b-7af489409572"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best GB parameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 200}\n",
            "Best GB score: 0.8653150722952255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression – Grid Search\n",
        "param_grid_lr = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10],\n",
        "    'classifier__penalty': ['l2'],\n",
        "    'classifier__solver': ['lbfgs']\n",
        "}\n",
        "\n",
        "lr_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=123))\n",
        "])\n",
        "\n",
        "grid_lr = GridSearchCV(lr_pipeline, param_grid_lr, cv=3, n_jobs=-1, scoring='accuracy')\n",
        "grid_lr.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best LR parameters:\", grid_lr.best_params_)\n",
        "print(\"Best LR score:\", grid_lr.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHExe_ROYIMK",
        "outputId": "645ea382-2e7f-490c-82ce-405694b5f9c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best LR parameters: {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
            "Best LR score: 0.846154068259699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing hyperparameter tuning using GridSearchCV, we now evaluate the optimized models on the validation set. The best models for Random Forest, Gradient Boosting, and Logistic Regression are used to predict the outcomes and their performance is assessed using accuracy, precision, recall, F1-score, and confusion matrix."
      ],
      "metadata": {
        "id": "hhO1vMvpeCeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best models\n",
        "best_rf = grid_rf.best_estimator_\n",
        "best_gb = grid_gb.best_estimator_\n",
        "best_lr = grid_lr.best_estimator_\n",
        "\n",
        "# Predict and evaluate\n",
        "for name, model in [('Random Forest', best_rf),\n",
        "                    ('Gradient Boosting', best_gb),\n",
        "                    ('Logistic Regression', best_lr)]:\n",
        "    y_pred = model.predict(X_val)\n",
        "    print(f\"\\n--- {name} (Tuned) ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    print(confusion_matrix(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDJGzCA3aDA-",
        "outputId": "2ca74218-da7b-45cd-fb91-a7704b00ef15"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Random Forest (Tuned) ---\n",
            "Accuracy: 0.8614366319444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.94      0.91      6946\n",
            "        >50K       0.77      0.62      0.69      2270\n",
            "\n",
            "    accuracy                           0.86      9216\n",
            "   macro avg       0.83      0.78      0.80      9216\n",
            "weighted avg       0.86      0.86      0.86      9216\n",
            "\n",
            "[[6528  418]\n",
            " [ 859 1411]]\n",
            "\n",
            "--- Gradient Boosting (Tuned) ---\n",
            "Accuracy: 0.8716362847222222\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.90      0.94      0.92      6946\n",
            "        >50K       0.78      0.67      0.72      2270\n",
            "\n",
            "    accuracy                           0.87      9216\n",
            "   macro avg       0.84      0.80      0.82      9216\n",
            "weighted avg       0.87      0.87      0.87      9216\n",
            "\n",
            "[[6521  425]\n",
            " [ 758 1512]]\n",
            "\n",
            "--- Logistic Regression (Tuned) ---\n",
            "Accuracy: 0.8490668402777778\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.93      0.90      6946\n",
            "        >50K       0.73      0.61      0.66      2270\n",
            "\n",
            "    accuracy                           0.85      9216\n",
            "   macro avg       0.81      0.77      0.78      9216\n",
            "weighted avg       0.84      0.85      0.84      9216\n",
            "\n",
            "[[6446  500]\n",
            " [ 891 1379]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the three tuned models, Gradient Boosting achieved the highest accuracy on the validation set (87.16%) and outperformed the others in detecting individuals earning over 50K, with a precision of 0.78, recall of 0.67, and F1-score of 0.72 for the positive class. These metrics indicate a better balance between false positives and false negatives compared to Random Forest and Logistic Regression.\n",
        "\n",
        "Although Random Forest performed comparably in terms of overall accuracy (86.14%) and had slightly higher precision (0.77), its lower recall (0.62) suggests it missed more high-income individuals. Logistic Regression, while simpler and faster to train, underperformed with the lowest accuracy (84.91%) and F1-score (0.66) on the >50K class.\n",
        "\n",
        "Therefore, Gradient Boosting is selected as the best-performing model for this binary classification task due to its superior balance of predictive performance across all key evaluation metrics."
      ],
      "metadata": {
        "id": "zQa7ZV9AaSyi"
      }
    }
  ]
}