{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50BMipsqISPu"
      },
      "source": [
        "# **OSHA Publications Scraper con Docling + VLM Integration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kdm2lSoM6FE8",
        "outputId": "98b6c70e-361c-4b42-934c-4162e8b117c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.4/251.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.5/164.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.8/963.8 kB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, Optional, Any, List\n",
        "import tempfile\n",
        "import os\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# Installazione dipendenze\n",
        "!pip install docling pymupdf pillow -q\n",
        "\n",
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions, PictureDescriptionVlmOptions\n",
        "from docling_core.types.doc import ImageRefMode\n",
        "\n",
        "# Per visualizzazione in Colab\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzEWlOy8I6Ut"
      },
      "source": [
        "### Global Configuration Settings\n",
        "\n",
        "This configuration class centralizes all scraper parameters, including directory paths for output files (PDFs, JSON metadata, extracted images and tables), HTTP request settings, and Vision Language Model (VLM) configuration for image description enrichment. We use this setup in order to easily modify parameters without searching the code and maintain consistency across all functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktG31jRV6JSX"
      },
      "outputs": [],
      "source": [
        "class ScraperConfig:\n",
        "    USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "    TIMEOUT = 30\n",
        "    OUTPUT_DIR = Path('./osha_scraped_data')\n",
        "    PDF_DIR = OUTPUT_DIR / 'pdfs'\n",
        "    JSON_DIR = OUTPUT_DIR / 'json'\n",
        "    IMAGES_DIR = OUTPUT_DIR / 'images'\n",
        "    TABLES_DIR = OUTPUT_DIR / 'tables'\n",
        "\n",
        "    # VLM Configuration\n",
        "    VLM_ENABLED = True  # Abilita VLM enrichment\n",
        "    VLM_MODEL = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
        "    VLM_PROMPT = \"Describe this image in detail. Focus on charts, diagrams, tables, and any visual data. Be technical and precise.\"\n",
        "\n",
        "    # This method ensures all necessary output directories exist before use.\n",
        "    @classmethod\n",
        "    def setup_directories(cls):\n",
        "        cls.OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "        cls.PDF_DIR.mkdir(exist_ok=True)\n",
        "        cls.JSON_DIR.mkdir(exist_ok=True)\n",
        "        cls.IMAGES_DIR.mkdir(exist_ok=True)\n",
        "        cls.TABLES_DIR.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjoUEqEFKzGa"
      },
      "source": [
        "### Docling Document Converter Initialization\n",
        "\n",
        "This function initializes the Docling DocumentConverter with configurable pipeline options for PDF processing. The optional choosen to work with are high-resolution image extraction (2x scale), OCR for scanned documents, and optionally activates a Vision Language Model (SmolVLM) to automatically generate textual descriptions of figures and charts found in the documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRE0y8Gu6TjQ"
      },
      "outputs": [],
      "source": [
        "def initialize_docling_converter(enable_vlm: bool = True):\n",
        "\n",
        "    pipeline_options = PdfPipelineOptions(\n",
        "        # Abilita l'estrazione di immagini\n",
        "        generate_picture_images=True,\n",
        "        images_scale=2.0,\n",
        "\n",
        "        # OCR, si attiva solo dove necessario\n",
        "        do_ocr=True,\n",
        "    )\n",
        "\n",
        "    # Se VLM è abilitato, aggiungi la configurazione\n",
        "    if enable_vlm and ScraperConfig.VLM_ENABLED:\n",
        "        print(f\"VLM abilitato: {ScraperConfig.VLM_MODEL}\")\n",
        "        pipeline_options.do_picture_description = True    # Abilita l'analisi delle immagini con AI\n",
        "        pipeline_options.picture_description_options = PictureDescriptionVlmOptions(\n",
        "            repo_id=ScraperConfig.VLM_MODEL,\n",
        "            prompt=ScraperConfig.VLM_PROMPT,\n",
        "        )\n",
        "\n",
        "    converter = DocumentConverter(\n",
        "        format_options={\n",
        "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Alla fine restituisce l'istanza configurata di DocumentConverter pronta per l'uso\n",
        "    return converter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAs4R7NqMb04"
      },
      "source": [
        "### PDF Download Function\n",
        "\n",
        "This function handles the HTTP download of PDF files from OSHA publication URLs. It uses streaming download (chunk-based) to efficiently handle large files, includes proper HTTP headers to avoid bot detection, implements timeout protection, and provides robust error handling with informative status messages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeAG_YREMck8"
      },
      "outputs": [],
      "source": [
        "def download_pdf(pdf_url: str, output_path: Path) -> bool:\n",
        "\n",
        "    try:\n",
        "        headers = {'User-Agent': ScraperConfig.USER_AGENT}\n",
        "        response = requests.get(\n",
        "            pdf_url,\n",
        "            headers=headers,\n",
        "            timeout=ScraperConfig.TIMEOUT,\n",
        "            stream=True    # Abilita lo streaming download, invece di caricare l'intero file in memoria, lo scarica a pezzi, chunks. Essenziale per PDF grandi, di anche 50-100 MB\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        print(f\"PDF scaricato: {output_path.name}\")\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Errore download PDF: {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPzfPI6nNJCW"
      },
      "source": [
        "### Image and Table Export Functions\n",
        "\n",
        "These utility functions extract and save images and tables from processed documents as separate files. Images are saved as PNG files in organized subdirectories, while tables are exported as TXT files containing the caption, column names, and full content. Both functions handle multiple extraction methods for compatibility across Docling versions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngeep5JVNJ-c"
      },
      "outputs": [],
      "source": [
        "# Esporta immagini tentando piu di un approccio, prima prova API nativa altrimenti itera manualmente sugli elementi\n",
        "# Risultato è un Dizionario per ogni immagine che include path, filename, caption per riferimento futuro\n",
        "# Esporta tutte le immagini dal documento gestendo ImageRef correttamente\n",
        "\n",
        "def export_images_from_document(doc, doc_id: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Esporta tutte le immagini dal documento - VERSIONE CORRETTA\n",
        "    Passa il documento a get_image() come nell'esempio Docling\n",
        "    \"\"\"\n",
        "    images_dir = ScraperConfig.IMAGES_DIR / doc_id\n",
        "    images_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    exported_images = []\n",
        "\n",
        "    try:\n",
        "        from docling_core.types.doc import PictureItem\n",
        "\n",
        "        # Itera sugli elementi del documento\n",
        "        picture_counter = 0\n",
        "\n",
        "        if hasattr(doc, 'iterate_items'):\n",
        "            print(f\"[DEBUG] Usando iterate_items()\")\n",
        "\n",
        "            for element, _level in doc.iterate_items():\n",
        "                if isinstance(element, PictureItem):\n",
        "                    picture_counter += 1\n",
        "                    img_filename = f\"figure_{picture_counter}.png\"\n",
        "                    img_path = images_dir / img_filename\n",
        "\n",
        "                    try:\n",
        "                        # CHIAVE: Passa doc a get_image()!\n",
        "                        pil_image = element.get_image(doc)\n",
        "                        pil_image.save(str(img_path), \"PNG\")\n",
        "\n",
        "                        exported_images.append({\n",
        "                            'path': str(img_path),\n",
        "                            'filename': img_filename,\n",
        "                            'caption': getattr(element, 'caption', 'N/A')\n",
        "                        })\n",
        "                        print(f\"  [+] Salvata: {img_filename}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  [!] Errore salvataggio figura {picture_counter}: {e}\")\n",
        "\n",
        "        # Fallback: usa doc.pictures se iterate_items non disponibile\n",
        "        elif hasattr(doc, 'pictures') and doc.pictures:\n",
        "            print(f\"[DEBUG] Usando doc.pictures con get_image(doc)\")\n",
        "\n",
        "            for idx, picture in enumerate(doc.pictures):\n",
        "                try:\n",
        "                    img_filename = f\"figure_{idx}.png\"\n",
        "                    img_path = images_dir / img_filename\n",
        "\n",
        "                    # Prova get_image con documento\n",
        "                    if hasattr(picture, 'get_image'):\n",
        "                        pil_image = picture.get_image(doc)  # ← Passa doc!\n",
        "                        pil_image.save(str(img_path), \"PNG\")\n",
        "\n",
        "                        exported_images.append({\n",
        "                            'path': str(img_path),\n",
        "                            'filename': img_filename,\n",
        "                            'caption': getattr(picture, 'caption', 'N/A')\n",
        "                        })\n",
        "                        print(f\"  [+] Salvata: {img_filename}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  [!] Errore figura {idx}: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(\"[WARNING] Nessun metodo di iterazione disponibile\")\n",
        "\n",
        "        print(f\"[RESULT] {len(exported_images)} immagini esportate in: {images_dir}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Export immagini fallito: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return exported_images\n",
        "\n",
        "\n",
        "def export_tables_to_files(tables: List[Dict], doc_id: str) -> List[str]:\n",
        "    # Esporta ogni tabella come formato TXT strutturato: Caption + Colonne + Contenuto in formato leggibile\n",
        "    # inoltre usiamo encoding='utf-8' importante per il supporto di caratteri internazionali (in questo caso documenti multilingua OSHA)\n",
        "    if not tables:\n",
        "        return []\n",
        "\n",
        "    tables_dir = ScraperConfig.TABLES_DIR / doc_id\n",
        "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    exported_tables = []\n",
        "\n",
        "    for table in tables:\n",
        "        table_id = table['table_id']\n",
        "        txt_path = tables_dir / f\"{table_id}.txt\"\n",
        "\n",
        "        try:\n",
        "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"Caption: {table['caption']}\\n\\n\")\n",
        "                if table.get('potential_columns'):\n",
        "                    f.write(f\"Columns: {', '.join(table['potential_columns'])}\\n\\n\")\n",
        "                f.write(\"Content:\\n\")\n",
        "                f.write(table['text_content'])\n",
        "\n",
        "            exported_tables.append(str(txt_path))\n",
        "            print(f\" Tabella salvata: {txt_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\" Errore salvataggio {table_id}: {e}\")\n",
        "\n",
        "    if exported_tables:\n",
        "        print(f\"{len(exported_tables)} tabelle esportate in: {tables_dir}\")\n",
        "\n",
        "    return exported_tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HNQxSdnOLuz"
      },
      "source": [
        "### Main PDF Processing with Docling\n",
        "\n",
        "This is the core processing function that orchestrates the complete document analysis pipeline. It uses Docling to convert PDFs into structured data, extracts text in both Markdown and plain formats, analyzes document structure (headings, tables, figures), exports images and tables as separate files, enriches figures with VLM-generated descriptions when available, and compiles comprehensive metadata including conversion quality scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPLXv-lQOa_j"
      },
      "outputs": [],
      "source": [
        "def process_pdf_with_docling(pdf_path: Path, converter: DocumentConverter, doc_id: str) -> Optional[Dict[str, Any]]:\n",
        "\n",
        "    try:\n",
        "        print(f\"Processing con Docling: {pdf_path.name}...\")\n",
        "\n",
        "        # Converti il documento\n",
        "        result = converter.convert(str(pdf_path))     # Punto di ingresso di Docling, analizza layout, estrae testo, applica OCR, genera descrizioni VLM.\n",
        "        doc = result.document\n",
        "\n",
        "        # Estrazione contenuto in formato Markdown e plain text, Markdown preserva struttura (heading, liste, tabelle), plain text è semplice stringa, entrambi utili in base al contesto\n",
        "        markdown_text = doc.export_to_markdown()\n",
        "        plain_text = doc.export_to_text()\n",
        "\n",
        "        # Analizza la struttura del documento\n",
        "        structure_info = analyze_document_structure(doc)\n",
        "\n",
        "        # Export immagini\n",
        "        exported_images = export_images_from_document(doc, doc_id)\n",
        "\n",
        "        # Arricchisci figure con VLM (se abilitato)\n",
        "        if ScraperConfig.VLM_ENABLED:\n",
        "            structure_info['figures'] = enrich_figures_with_vlm_annotations(\n",
        "                doc,\n",
        "                structure_info['figures']\n",
        "            )\n",
        "\n",
        "        # Export tabelle\n",
        "        exported_tables = export_tables_to_files(structure_info['tables'], doc_id)\n",
        "\n",
        "        # Compila metadati strutturali completi\n",
        "        document_data = {\n",
        "            'markdown_content': markdown_text,\n",
        "            'plain_text': plain_text,\n",
        "\n",
        "            'num_pages': len(doc.pages) if hasattr(doc, 'pages') else 'N/A',\n",
        "            'num_tables': structure_info['num_tables'],\n",
        "            'num_figures': structure_info['num_figures'],\n",
        "            'num_headings': structure_info['num_headings'],\n",
        "\n",
        "            'headings': structure_info['headings'],\n",
        "            'tables': structure_info['tables'],\n",
        "            'figures': structure_info['figures'],\n",
        "\n",
        "            'exported_images': exported_images,\n",
        "            'exported_tables': exported_tables,\n",
        "\n",
        "            'stats': {\n",
        "                'total_chars': len(plain_text),\n",
        "                'total_chars_markdown': len(markdown_text),\n",
        "                'has_structured_content': structure_info['num_tables'] > 0 or structure_info['num_figures'] > 0\n",
        "            },\n",
        "\n",
        "            'conversion_quality': get_conversion_quality(result)\n",
        "        }\n",
        "\n",
        "        print(f\"Docling processing completato:\")\n",
        "        print(f\"  - {document_data['num_pages']} pagine\")\n",
        "        print(f\"  - {document_data['num_tables']} tabelle\")\n",
        "        print(f\"  - {document_data['num_figures']} figure\")\n",
        "        print(f\"  - {document_data['num_headings']} headings\")\n",
        "        print(f\"  - {len(exported_images)} immagini esportate\")\n",
        "        print(f\"  - {len(exported_tables)} tabelle esportate\")\n",
        "\n",
        "        return document_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore processing Docling: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "def enrich_figures_with_vlm_annotations(doc, figures_metadata: List[Dict]) -> List[Dict]:\n",
        "    # Cerca annotazioni già generate da Docling durante la conversione, non chiama VLM direttamente in quanto Docling potrebbe averlo già fatto se configurato.\n",
        "    enriched_figures = []\n",
        "\n",
        "    try:\n",
        "        for fig_meta in figures_metadata:\n",
        "            enriched_fig = fig_meta.copy()\n",
        "\n",
        "            # Cerca annotazioni VLM generate da Docling\n",
        "            if hasattr(doc, 'main_text'):\n",
        "                for item in doc.main_text:\n",
        "                    if hasattr(item, 'annotations'):\n",
        "                        for annotation in item.annotations:\n",
        "                            annotation_type = type(annotation).__name__\n",
        "                            if 'PictureDescription' in annotation_type or 'Description' in annotation_type:\n",
        "                                if hasattr(annotation, 'text'):\n",
        "                                    enriched_fig['vlm_description'] = annotation.text\n",
        "                                    enriched_fig['vlm_model'] = ScraperConfig.VLM_MODEL\n",
        "                                    print(f\" Descrizione VLM trovata per {fig_meta['figure_id']}\")\n",
        "                                elif hasattr(annotation, 'description'):\n",
        "                                    enriched_fig['vlm_description'] = annotation.description\n",
        "                                    enriched_fig['vlm_model'] = ScraperConfig.VLM_MODEL\n",
        "\n",
        "            enriched_figures.append(enriched_fig)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Avviso durante enrichment VLM: {e}\")\n",
        "        enriched_figures = figures_metadata\n",
        "\n",
        "    return enriched_figures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw2YeJlePZXt"
      },
      "source": [
        "### Document Structure Analysis and Metadata Extraction\n",
        "\n",
        "These utility functions analyze the internal structure of Docling documents to identify and extract headings, tables, and figures. The system implements a multi-tier fallback strategy: first attempting to parse Docling's internal representation, then falling back to regex-based Markdown parsing if needed. Additional helper functions extract detailed metadata from tables (columns, captions) and figures, and retrieve conversion quality scores when available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydoKm2UpQJw9"
      },
      "outputs": [],
      "source": [
        "# Funzione principale che cerca di estrarre la struttura del documento usando l'API interna di Docling\n",
        "def analyze_document_structure(doc) -> Dict[str, Any]:\n",
        "\n",
        "    structure = {\n",
        "        'num_tables': 0, 'num_figures': 0, 'num_headings': 0,    # Usiamo dei contatori per spaere quante tabelle/figure/titoli ci sono\n",
        "        'headings': [], 'tables': [], 'figures': []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Tenta accesso struttura interna Docling, prova prima main_text, se non esiste prova body.children, facciamo una cascata di tentativi\n",
        "        items = None\n",
        "        if hasattr(doc, 'main_text'):\n",
        "            items = doc.main_text\n",
        "        elif hasattr(doc, 'body') and hasattr(doc.body, 'children'):\n",
        "            items = doc.body.children\n",
        "\n",
        "        # Fallback se struttura non accessibile\n",
        "        if items is None or (hasattr(items, '__len__') and len(list(items)) == 0):\n",
        "            print(\"Struttura interna non disponibile, usando metodo alternativo\")\n",
        "            return extract_structure_from_markdown(doc)\n",
        "\n",
        "        # Itera sugli elementi, quindi prepara e identifica ogni elemento del documento\n",
        "        items_processed = 0\n",
        "        for idx, item in enumerate(items):\n",
        "            items_processed += 1\n",
        "            item_label = getattr(item, 'label', None) or type(item).__name__\n",
        "            item_label_lower = str(item_label).lower()\n",
        "            item_text = getattr(item, 'text', '')\n",
        "\n",
        "            if 'table' in item_label_lower:\n",
        "                structure['num_tables'] += 1\n",
        "                structure['tables'].append(extract_table_metadata(item, idx))\n",
        "            elif any(kw in item_label_lower for kw in ['picture', 'figure', 'image']):\n",
        "                structure['num_figures'] += 1\n",
        "                structure['figures'].append(extract_figure_metadata(item, idx))\n",
        "            elif any(kw in item_label_lower for kw in ['heading', 'title', 'section']):\n",
        "                structure['num_headings'] += 1\n",
        "                structure['headings'].append({\n",
        "                    'type': item_label, 'text': item_text[:200], 'position': idx\n",
        "                })\n",
        "\n",
        "        # Se non trova nulla, usa fallback, ed attiva il fallback che analizza il Markdown che è sempre affidabile\n",
        "        if all(structure[k] == 0 for k in ['num_tables', 'num_figures', 'num_headings']):\n",
        "            print(f\"Nessun elemento trovato in {items_processed} items, fallback markdown\")\n",
        "            return extract_structure_from_markdown(doc)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore analisi struttura: {e}\")\n",
        "        try:\n",
        "            return extract_structure_from_markdown(doc)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return structure\n",
        "\n",
        "\n",
        "def extract_structure_from_markdown(doc) -> Dict[str, Any]:\n",
        "    # Fallback estrae struttura da Markdown con regex e pattern matching\n",
        "    structure = {\n",
        "        'num_tables': 0, 'num_figures': 0, 'num_headings': 0,\n",
        "        'headings': [], 'tables': [], 'figures': []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        markdown = doc.export_to_markdown()\n",
        "        lines = markdown.split('\\n')\n",
        "        in_table = False\n",
        "        current_table_lines = []\n",
        "        table_start_idx = -1\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            # Riconoscimento Headings, ovvero linee che iniziano con # e rimuove spazi bianchi all'inizio/fine\n",
        "            if line.strip().startswith('#'):\n",
        "                level = len(line) - len(line.lstrip('#'))\n",
        "                text = line.lstrip('#').strip()\n",
        "                structure['num_headings'] += 1\n",
        "                structure['headings'].append({\n",
        "                    'type': f'heading_level_{level}',\n",
        "                    'text': text[:200],\n",
        "                    'position': i,\n",
        "                    'level': level\n",
        "                })\n",
        "\n",
        "            # Riconoscimento Tabelle, cerca righe con | e separatori ---\n",
        "            if '|' in line and ('-' in line or '─' in line):\n",
        "                if not in_table:\n",
        "                    in_table = True\n",
        "                    table_start_idx = i\n",
        "                    current_table_lines = []\n",
        "\n",
        "            # accumula tutte le righe di una tabella fino a trovare una riga che non fa parte della tabella\n",
        "            if in_table:\n",
        "                if '|' in line:\n",
        "                    current_table_lines.append(line)\n",
        "                else:\n",
        "                    if current_table_lines:\n",
        "                        structure['num_tables'] += 1\n",
        "                        caption = 'N/A'\n",
        "                        for j in range(max(0, table_start_idx-3), table_start_idx):\n",
        "                            if lines[j].strip() and not lines[j].strip().startswith('#'):\n",
        "                                caption = lines[j].strip()\n",
        "                                break\n",
        "                        structure['tables'].append({\n",
        "                            'table_id': f'table_{structure[\"num_tables\"]-1}',\n",
        "                            'caption': caption,\n",
        "                            'text_content': '\\n'.join(current_table_lines),\n",
        "                            'num_rows': len(current_table_lines),\n",
        "                            'position': table_start_idx\n",
        "                        })\n",
        "                    in_table = False\n",
        "                    current_table_lines = []\n",
        "\n",
        "            # Riconoscimento Figure (<!-- image --> o ![alt](url))\n",
        "            if '<!-- image -->' in line.lower() or (line.strip().startswith('![') and '](' in line):\n",
        "                structure['num_figures'] += 1\n",
        "                caption = 'N/A'\n",
        "                # Ricerca caption nelle righe successive, controlla le 3 righe successive alla figura\n",
        "                for j in range(i+1, min(len(lines), i+4)):\n",
        "                    if lines[j].strip() and not lines[j].strip().startswith('#'):\n",
        "                        caption = lines[j].strip()\n",
        "                        break      # Esce dal loop appena trova una caption, non continua a cercare\n",
        "                structure['figures'].append({\n",
        "                    'figure_id': f'figure_{structure[\"num_figures\"]-1}',\n",
        "                    'caption': caption[:200],\n",
        "                    'position': i\n",
        "                })\n",
        "\n",
        "        # Gestisci tabella che finisce a fine file\n",
        "        # Problema: Se il documento finisce mentre siamo in una tabella, il loop termina senza salvare l'ultima tabella\n",
        "        # Soluzione: Dopo il loop, controlla se in_table == True e se ci sono righe accumulate, se sì, salva la tabella\n",
        "        if in_table and current_table_lines:\n",
        "            structure['num_tables'] += 1\n",
        "            structure['tables'].append({\n",
        "                'table_id': f'table_{structure[\"num_tables\"]-1}',\n",
        "                'caption': 'End of document table',\n",
        "                'text_content': '\\n'.join(current_table_lines),\n",
        "                'num_rows': len(current_table_lines),\n",
        "                'position': table_start_idx\n",
        "            })\n",
        "\n",
        "        print(f\"Struttura da Markdown: {structure['num_headings']} headings, {structure['num_tables']} tabelle, {structure['num_figures']} figure\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore fallback markdown: {e}\")\n",
        "\n",
        "    return structure\n",
        "\n",
        "# Estrae metadati dettagliati da una tabella Docling\n",
        "def extract_table_metadata(table_item, index: int) -> Dict[str, Any]:\n",
        "\n",
        "    table_data = {\n",
        "        'table_id': f\"table_{index}\",\n",
        "        'caption': getattr(table_item, 'caption', 'N/A'),\n",
        "        'text_content': getattr(table_item, 'text', ''),\n",
        "    }\n",
        "    try:\n",
        "        if hasattr(table_item, 'export_to_markdown'):\n",
        "            table_data['markdown_content'] = table_item.export_to_markdown()    # Alcune versioni Docling permettono export Markdown diretto della tabella. Se disponibile, lo usa.\n",
        "        text_lines = table_data['text_content'].split('\\n')\n",
        "        if text_lines:\n",
        "            table_data['potential_columns'] = [c.strip() for c in text_lines[0].split('|') if c.strip()]     # Assume che la prima riga contenga i nomi delle colonne (convenzione Markdown).\n",
        "        table_data['num_rows_estimated'] = len(text_lines)\n",
        "    except Exception as e:\n",
        "        print(f\"Errore metadati tabella {index}: {e}\")\n",
        "    return table_data\n",
        "\n",
        "# Crea e restituisce il dizionario in un solo step\n",
        "def extract_figure_metadata(figure_item, index: int) -> Dict[str, Any]:\n",
        "\n",
        "    return {\n",
        "        'figure_id': f\"figure_{index}\",\n",
        "        'caption': getattr(figure_item, 'caption', 'N/A'),\n",
        "        'alt_text': getattr(figure_item, 'text', 'N/A'),\n",
        "        'vlm_description': None,\n",
        "        'image_path': None\n",
        "    }\n",
        "\n",
        "# Estrae quality scores introdotti nelle nuove versioni di Docling\n",
        "def get_conversion_quality(result) -> Dict[str, Any]:\n",
        "\n",
        "    quality_info = {'status': 'completed', 'has_confidence_scores': False}    # Anche se tutto fallisce, abbiamo almeno status: 'completed'\n",
        "\n",
        "    try:\n",
        "        if hasattr(result, 'confidence') and result.confidence:\n",
        "            quality_info['has_confidence_scores'] = True\n",
        "            quality_info['mean_grade'] = str(getattr(result.confidence, 'mean_grade', 'N/A'))        # Media di tutti i punteggi (overall quality)\n",
        "            quality_info['low_grade'] = str(getattr(result.confidence, 'low_grade', 'N/A'))          # 5° percentile (worst areas)\n",
        "            quality_info['layout_score'] = str(getattr(result.confidence, 'layout_score', 'N/A'))    # Qualità riconoscimento layout\n",
        "            quality_info['ocr_score'] = str(getattr(result.confidence, 'ocr_score', 'N/A'))          # Qualità OCR\n",
        "    except Exception as e:\n",
        "        print(f\" Errore quality scores: {e}\")\n",
        "    return quality_info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjczuOH9VLKU"
      },
      "source": [
        "Up to this point, the work has been set up with 3 levels of fallback which ensure that it always functions, even with different Docling versions or unusual documents. This guarantees we extract the maximum information possible: It extracts everything—text, metadata, coordinates (positions), and captions. Furthermore, we have complete error handling—it never crashes, providing partial data at worst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3XqTgpeVg9b"
      },
      "source": [
        "### Main Scraping Function\n",
        "\n",
        "This is the top-level function that coordinates the complete scraping and processing workflow for OSHA publications. It manages three distinct phases:\n",
        "\n",
        "\n",
        "1. Web scraping to extract metadata from the publication webpage using BeautifulSoup with custom CSS selectors for OSHA's specific HTML structure\n",
        "2. PDF download and Docling-based document processing with VLM enrichment.\n",
        "3. Data consolidation into a comprehensive JSON structure.\n",
        "\n",
        "The function implements robust error handling, returns structured metadata including scraping timestamp and version tracking, and provides boolean flags for workflow control. Optional PDF retention allows for disk space management in large-scale batch operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEYeuFOG6dI7"
      },
      "outputs": [],
      "source": [
        "def scrape_osha_publication(\n",
        "    publication_url: str,\n",
        "    converter: DocumentConverter,\n",
        "    save_pdf: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    print(f\"\\n{'='*80}\")   # separatore visivo\n",
        "    print(f\"Scraping pubblicazione: {publication_url}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    scraping_metadata = {\n",
        "        'url': publication_url,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'scraper_version': '3.0',\n",
        "        'docling_used': True,\n",
        "        'vlm_enabled': ScraperConfig.VLM_ENABLED\n",
        "    }\n",
        "\n",
        "    # Estrazione Metadati con Selettori specifici per OSHA\n",
        "    try:\n",
        "        headers = {'User-Agent': ScraperConfig.USER_AGENT}\n",
        "        response = requests.get(publication_url, headers=headers, timeout=ScraperConfig.TIMEOUT)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        page = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "        print(\"Estrazione metadati dalla pagina web...\")\n",
        "\n",
        "        title_elem = page.find('h1')\n",
        "        title = title_elem.text.strip() if title_elem else 'N/A'\n",
        "\n",
        "        keywords_elem = page.find('ul', class_='field__items')\n",
        "        keywords = [kw.strip() for kw in keywords_elem.text.strip().split('\\n')] if keywords_elem else []\n",
        "\n",
        "        descr_blocks = page.find_all(id=re.compile('^tmgmt-'))\n",
        "        description = ' '.join(p.get_text(strip=True, separator=' ') for p in descr_blocks)\n",
        "        description = re.sub(r'\\s+', ' ', description.replace('\\xa0', ' ')).strip()\n",
        "\n",
        "        date_elem = page.find('p', class_='datetime-style')\n",
        "        publication_date = date_elem.text.strip() if date_elem else 'N/A'\n",
        "\n",
        "        link_pdf_elem = page.find('div', class_='download-pdf')\n",
        "        pdf_url = link_pdf_elem.find('a')['href'] if link_pdf_elem and link_pdf_elem.find('a') else None\n",
        "\n",
        "        print(f\"Titolo: {title}\")\n",
        "        print(f\"Data: {publication_date}\")\n",
        "        print(f\"Keywords: {', '.join(keywords)}\")\n",
        "\n",
        "        pdf_data = None\n",
        "        pdf_local_path = None\n",
        "\n",
        "        # Processing PDF Condizionale\n",
        "        if pdf_url:\n",
        "            safe_filename = re.sub(r'[^\\w\\-_.]', '_', title[:50]) + '.pdf'\n",
        "            pdf_local_path = ScraperConfig.PDF_DIR / safe_filename\n",
        "            doc_id = safe_filename.replace('.pdf', '')\n",
        "\n",
        "            if download_pdf(pdf_url, pdf_local_path):\n",
        "                pdf_data = process_pdf_with_docling(pdf_local_path, converter, doc_id)\n",
        "\n",
        "                if not save_pdf:\n",
        "                    pdf_local_path.unlink()    # elaborazione di un PDF scaricato, includendo una fase opzionale per eliminare il file locale subito dopo averlo processato\n",
        "                    pdf_local_path = None      # ottimizzare l'uso dello spazio disco, specialmente nel trattamento batch di un gran numero di documenti\n",
        "        else:\n",
        "            print(\"Nessun PDF disponibile per questa pubblicazione\")\n",
        "\n",
        "        # Costruzione Dizionario Risultato\n",
        "        publication_data = {\n",
        "            'scraping_metadata': scraping_metadata,\n",
        "\n",
        "            'web_metadata': {\n",
        "                'title': title,\n",
        "                'publication_date': publication_date,\n",
        "                'keywords': keywords,\n",
        "                'description': description,\n",
        "                'pdf_url': pdf_url,\n",
        "                'pdf_local_path': str(pdf_local_path) if pdf_local_path else None\n",
        "            },\n",
        "\n",
        "            'document_content': pdf_data if pdf_data else None,\n",
        "\n",
        "            'status': 'success',\n",
        "            'has_pdf': pdf_url is not None,\n",
        "            'pdf_processed': pdf_data is not None\n",
        "        }\n",
        "\n",
        "        print(f\"\\n Scraping completato con successo!\")\n",
        "        return publication_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Errore durante lo scraping: {e}\")\n",
        "        return {\n",
        "            'scraping_metadata': scraping_metadata,\n",
        "            'status': 'error',\n",
        "            'error_message': str(e)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlbrHon_Z4Pr"
      },
      "source": [
        "### Saving and Visualization Functions\n",
        "\n",
        "These utility functions ensure consistent data storage and reproducible presentation of results within Jupyter or Colab environments.\n",
        "The save_to_json function serializes the full scraping output into a structured JSON file. Filenames are automatically generated using the publication title and a timestamp, while UTF-8 encoding guarantees compatibility with multilingual text. The resulting files are indented for improved readability and downstream inspection.\n",
        "\n",
        "The display_results_in_colab function generates an interactive HTML summary designed to facilitate exploratory analysis and quality assessment. It presents key processing statistics, a hierarchical overview of the document structure, extracted tables and figures accompanied by VLM-generated descriptions, a Markdown rendering of the content, and organized export options for all derived files.\n",
        "\n",
        "This combination of machine-readable storage (JSON) and human-readable visualization (HTML) supports both automated integration in data workflows and transparent evaluation of extraction outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qrb_-Ky_6iIL"
      },
      "outputs": [],
      "source": [
        "def save_to_json(data: Dict[str, Any], output_name: Optional[str] = None) -> Path:\n",
        "    # Salva i dati in formato JSON, restituisce un oggetto Path che rappresenta il file salvato\n",
        "    if output_name is None:\n",
        "        title = data.get('web_metadata', {}).get('title', 'unknown')    # Cerca accesso sicuro a dizionario annidato se presenta, web_metadata è un dizionario (dictionary) Python che contiene tutti i metadati estratti dalla pagina web\n",
        "        safe_title = re.sub(r'[^\\w\\-_.]', '_', title[:50])              # Cerca title dentro\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')            # Formatta datetime come stringa\n",
        "        output_name = f\"{safe_title}_{timestamp}.json\"\n",
        "\n",
        "    output_path = ScraperConfig.JSON_DIR / output_name\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)                # ensure_ascii=False parametro che istruisce a non convertire i caratteri non-ASCII,\n",
        "                                                                        # indent=2 enables 'pretty print' formatting (newlines + 2 spaces), making the JSON output much clearer and human-readable.\n",
        "    print(f\"Dati salvati in: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def display_results_in_colab(result: Dict[str, Any], json_path: Path):\n",
        "\n",
        "    # Visualizza i risultati in modo interattivo su Colab, CSS styling\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"VISUALIZZAZIONE RISULTATI\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # 1) Link per scaricare il JSON\n",
        "    display(HTML(f\"\"\"\n",
        "    <div style=\"padding: 20px; background-color: #f0f0f0; border-radius: 10px; margin: 10px 0;\">\n",
        "        <h3>File JSON Generato</h3>\n",
        "        <p><strong>Path:</strong> <code>{json_path}</code></p>\n",
        "        <p>Per scaricare: Click destro sul file nel pannello Files → Download</p>\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "\n",
        "    # 2) Informazioni principali\n",
        "    if result['status'] == 'success' and result['document_content']:\n",
        "        doc_content = result['document_content']\n",
        "        web_meta = result['web_metadata']\n",
        "\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style=\"padding: 20px; background-color: #e8f5e9; border-radius: 10px; margin: 10px 0;\">\n",
        "            <h3>Documento Processato con Successo</h3>\n",
        "            <p><strong>Titolo:</strong> {web_meta['title']}</p>\n",
        "            <p><strong>Data:</strong> {web_meta['publication_date']}</p>\n",
        "            <p><strong>Keywords:</strong> {', '.join(web_meta['keywords'])}</p>\n",
        "            <hr>\n",
        "            <p><strong>Pagine:</strong> {doc_content['num_pages']}</p>\n",
        "            <p><strong>Caratteri:</strong> {doc_content['stats']['total_chars']:,}</p>\n",
        "            <p><strong>Headings:</strong> {doc_content['num_headings']}</p>\n",
        "            <p><strong>Tabelle:</strong> {doc_content['num_tables']}</p>\n",
        "            <p><strong>Figure:</strong> {doc_content['num_figures']}</p>\n",
        "            <p><strong>Immagini esportate:</strong> {len(doc_content.get('exported_images', []))}</p>\n",
        "            <p><strong>Tabelle esportate:</strong> {len(doc_content.get('exported_tables', []))}</p>\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "        # 3) Mostra headings\n",
        "        if doc_content['headings']:\n",
        "            print(\"\\n Struttura del Documento (Headings):\\n\")\n",
        "            for h in doc_content['headings'][:10]:\n",
        "                level = h.get('level', 2)\n",
        "                indent = \"  \" * (level - 1)\n",
        "                print(f\"{indent}{'#' * level} {h['text'][:80]}\")\n",
        "\n",
        "        # 4) Mostra tabelle\n",
        "        if doc_content['tables']:\n",
        "            print(f\"\\n Tabelle Trovate ({len(doc_content['tables'])}):\\n\")\n",
        "            for table in doc_content['tables']:\n",
        "                print(f\"  • {table['table_id']}: {table['caption']}\")\n",
        "                if table.get('potential_columns'):\n",
        "                    print(f\"    Colonne: {', '.join(table['potential_columns'][:5])}\")\n",
        "\n",
        "        # 5) Mostra figure con descrizioni VLM\n",
        "        if doc_content['figures']:\n",
        "            print(f\"\\n Figure Trovate ({len(doc_content['figures'])}):\\n\")\n",
        "            for fig in doc_content['figures']:\n",
        "                print(f\"  • {fig['figure_id']}: {fig['caption']}\")\n",
        "                if fig.get('vlm_description'):\n",
        "                    print(f\"  VLM: {fig['vlm_description'][:150]}...\")\n",
        "                    print()\n",
        "\n",
        "        # 6) Mostra anteprima markdown\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"ANTEPRIMA MARKDOWN (prime 800 caratteri)\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        print(doc_content['markdown_content'][:800])\n",
        "        print(\"\\n[...]\\n\")\n",
        "\n",
        "        # 7) Lista file esportati\n",
        "        if doc_content.get('exported_images') or doc_content.get('exported_tables'):\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(\"FILE ESPORTATI\")\n",
        "            print(f\"{'='*80}\\n\")\n",
        "\n",
        "            if doc_content.get('exported_images'):\n",
        "                print(f\"Immagini ({len(doc_content['exported_images'])}):\")\n",
        "                for img in doc_content['exported_images'][:5]:\n",
        "                    print(f\"  - {img.get('filename', img.get('path', 'N/A'))}\")\n",
        "\n",
        "            if doc_content.get('exported_tables'):\n",
        "                print(f\"\\n Tabelle ({len(doc_content['exported_tables'])}):\")\n",
        "                for table_path in doc_content['exported_tables']:\n",
        "                    print(f\"  - {Path(table_path).name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CILwEYuXdfcm"
      },
      "source": [
        "### Main Execution Block and Usage Example\n",
        "\n",
        "This is the entry point of the scraping system, demonstrating a complete end-to-end workflow execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LXDhtXhR6r37",
        "outputId": "b86b7159-e99c-48e8-8ef1-1029dc539187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VLM abilitato: HuggingFaceTB/SmolVLM-256M-Instruct\n",
            "\n",
            "================================================================================\n",
            "Scraping pubblicazione: https://osha.europa.eu/it/publications/health-and-social-care-workers-free-musculoskeletal-disorders-awareness-campaign\n",
            "================================================================================\n",
            "\n",
            "Estrazione metadati dalla pagina web...\n",
            "Titolo: «Per lavoratori del settore dell’assistenza socio-sanitaria senza disturbi muscolo-scheletrici»: una campagna di sensibilizzazione\n",
            "Data: 03/10/2025\n",
            "Keywords: Assistenza sociosanitaria, Disturbi muscoloscheletrici\n",
            "PDF scaricato: _Per_lavoratori_del_settore_dell_assistenza_socio-.pdf\n",
            "Processing con Docling: _Per_lavoratori_del_settore_dell_assistenza_socio-.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[INFO] 2025-10-22 16:25:35,344 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-22 16:25:35,385 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-22 16:25:35,386 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-22 16:25:35,595 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-22 16:25:35,599 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-22 16:25:35,600 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-22 16:25:35,688 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-22 16:25:35,763 [RapidOCR] download_file.py:60: File exists and is valid: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-22 16:25:35,764 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "WARNING:docling_core.types.doc.document:Parameter `strict_text` has been deprecated and will be ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nessun elemento trovato in 139 items, fallback markdown\n",
            "Struttura da Markdown: 24 headings, 1 tabelle, 8 figure\n",
            "[DEBUG] Usando iterate_items()\n",
            "  [+] Salvata: figure_1.png\n",
            "  [+] Salvata: figure_2.png\n",
            "  [+] Salvata: figure_3.png\n",
            "  [+] Salvata: figure_4.png\n",
            "  [+] Salvata: figure_5.png\n",
            "  [+] Salvata: figure_6.png\n",
            "  [+] Salvata: figure_7.png\n",
            "  [+] Salvata: figure_8.png\n",
            "[RESULT] 8 immagini esportate in: osha_scraped_data/images/_Per_lavoratori_del_settore_dell_assistenza_socio-\n",
            " Tabella salvata: table_0.txt\n",
            "1 tabelle esportate in: osha_scraped_data/tables/_Per_lavoratori_del_settore_dell_assistenza_socio-\n",
            "Docling processing completato:\n",
            "  - 8 pagine\n",
            "  - 1 tabelle\n",
            "  - 8 figure\n",
            "  - 24 headings\n",
            "  - 8 immagini esportate\n",
            "  - 1 tabelle esportate\n",
            "\n",
            " Scraping completato con successo!\n",
            "Dati salvati in: osha_scraped_data/json/_Per_lavoratori_del_settore_dell_assistenza_socio-_20251022_163453.json\n",
            "\n",
            "================================================================================\n",
            "VISUALIZZAZIONE RISULTATI\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style=\"padding: 20px; background-color: #f0f0f0; border-radius: 10px; margin: 10px 0;\">\n",
              "        <h3>File JSON Generato</h3>\n",
              "        <p><strong>Path:</strong> <code>osha_scraped_data/json/_Per_lavoratori_del_settore_dell_assistenza_socio-_20251022_163453.json</code></p>\n",
              "        <p>Per scaricare: Click destro sul file nel pannello Files → Download</p>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"padding: 20px; background-color: #e8f5e9; border-radius: 10px; margin: 10px 0;\">\n",
              "            <h3>Documento Processato con Successo</h3>\n",
              "            <p><strong>Titolo:</strong> «Per lavoratori del settore dell’assistenza socio-sanitaria senza disturbi muscolo-scheletrici»: una campagna di sensibilizzazione</p>\n",
              "            <p><strong>Data:</strong> 03/10/2025</p>\n",
              "            <p><strong>Keywords:</strong> Assistenza sociosanitaria, Disturbi muscoloscheletrici</p>\n",
              "            <hr>\n",
              "            <p><strong>Pagine:</strong> 8</p>\n",
              "            <p><strong>Caratteri:</strong> 23,532</p>\n",
              "            <p><strong>Headings:</strong> 24</p>\n",
              "            <p><strong>Tabelle:</strong> 1</p>\n",
              "            <p><strong>Figure:</strong> 8</p>\n",
              "            <p><strong>Immagini esportate:</strong> 8</p>\n",
              "            <p><strong>Tabelle esportate:</strong> 1</p>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Struttura del Documento (Headings):\n",
            "\n",
            "  ## 'FOR HEALTH AND SOCIAL CARE WORKERS FREE FROM MUSCULOSKELETAL DISORDERS' - AN AW\n",
            "  ## Introduction\n",
            "  ## Description of the case\n",
            "  ## Aim\n",
            "  ## What was done and how\n",
            "  ## Preparation of the plan\n",
            "  ## Focus on the residential care sector\n",
            "  ## Campaign material 14\n",
            "  ##  Videos\n",
            "  ##  Posters\n",
            "\n",
            " Tabelle Trovate (1):\n",
            "\n",
            "  • table_0: 20 Impressions are the number of times a post is seen on the platform.\n",
            "\n",
            " Figure Trovate (8):\n",
            "\n",
            "  • figure_0: N/A\n",
            "  • figure_1: N/A\n",
            "  • figure_2: <!-- image -->\n",
            "  • figure_3: <!-- image -->\n",
            "  • figure_4: These posters were available in PDF and in paper format. In the case of the latter, they were made available to their networks by the various members of the CNSST working group.\n",
            "  • figure_5: N/A\n",
            "  • figure_6: N/A\n",
            "  • figure_7: N/A\n",
            "\n",
            "================================================================================\n",
            "ANTEPRIMA MARKDOWN (prime 800 caratteri)\n",
            "================================================================================\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "## 'FOR HEALTH AND SOCIAL CARE WORKERS FREE FROM MUSCULOSKELETAL DISORDERS' - AN AWARENESS CAMPAIGN\n",
            "\n",
            "## Introduction\n",
            "\n",
            "Work-related musculoskeletal disorders (MSDs) are a significant problem in Spain, accounting for 30% 1 of  occupational  accidents  in  general  and  more  than  60%  in  the  health  and  social  care  (HeSCare) sector. 2  In addition, 80% of reported occupational diseases are related to MSDs. 3  Between 2018 and 2020, the incidence rates of occupational accidents in the HeSCare sector was 1.7 times the average when compared to all sectors in the economy. 4  This situation has a major economic and social impact, as many potentially healthy workers leave the labour market due to these disorders.\n",
            "\n",
            "An awareness campaign - 'For health and social care workers fr\n",
            "\n",
            "[...]\n",
            "\n",
            "\n",
            "================================================================================\n",
            "FILE ESPORTATI\n",
            "================================================================================\n",
            "\n",
            "Immagini (8):\n",
            "  - figure_1.png\n",
            "  - figure_2.png\n",
            "  - figure_3.png\n",
            "  - figure_4.png\n",
            "  - figure_5.png\n",
            "\n",
            " Tabelle (1):\n",
            "  - table_0.txt\n",
            "\n",
            "================================================================================\n",
            "PROCESSO COMPLETATO!\n",
            "================================================================================\n",
            "\n",
            " Trova tutti i file in:\n",
            "  - JSON: osha_scraped_data/json\n",
            "  - PDF: osha_scraped_data/pdfs\n",
            "  - Immagini: osha_scraped_data/images\n",
            "  - Tabelle: osha_scraped_data/tables\n",
            "\n",
            " Usa il pannello Files (sinistra) per navigare e scaricare i file\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup\n",
        "    ScraperConfig.setup_directories()\n",
        "\n",
        "    # Inizializza converter con VLM\n",
        "    converter = initialize_docling_converter(enable_vlm=True)\n",
        "\n",
        "    # URL di test\n",
        "    test_url = 'https://osha.europa.eu/it/publications/health-and-social-care-workers-free-musculoskeletal-disorders-awareness-campaign'\n",
        "    # https://osha.europa.eu/it/publications/tms-pros-programme-supporting-msds-prevention-health-and-social-care-sector\n",
        "    # Esegui scraping\n",
        "    result = scrape_osha_publication(\n",
        "        publication_url=test_url,\n",
        "        converter=converter,\n",
        "        save_pdf=True\n",
        "    )\n",
        "\n",
        "    # Salva risultati\n",
        "    json_path = save_to_json(result)\n",
        "\n",
        "    # Visualizza risultati in Colab\n",
        "    display_results_in_colab(result, json_path)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"PROCESSO COMPLETATO!\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\n Trova tutti i file in:\")\n",
        "    print(f\"  - JSON: {ScraperConfig.JSON_DIR}\")\n",
        "    print(f\"  - PDF: {ScraperConfig.PDF_DIR}\")\n",
        "    print(f\"  - Immagini: {ScraperConfig.IMAGES_DIR}\")\n",
        "    print(f\"  - Tabelle: {ScraperConfig.TABLES_DIR}\")\n",
        "    print(f\"\\n Usa il pannello Files (sinistra) per navigare e scaricare i file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keyword-Based Publication Discovery\n",
        "\n",
        "Builds OSHA search URLs from user queries and extracts publication links across multiple result pages. Implements anti-detection measures (realistic headers, progressive delays, session cookies) and dual parsing strategies to reliably collect unique publication URLs while avoiding rate limiting. Includes automatic pagination handling and duplicate removal.\n"
      ],
      "metadata": {
        "id": "x4APFQRc0_A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import quote_plus\n",
        "import unicodedata\n",
        "import time\n",
        "import json\n",
        "\n",
        "def build_osha_search_url(query: str, lang: str, sort: str = \"field_publication_date\") -> str:\n",
        "\n",
        "    if not isinstance(query, str) or not query.strip():\n",
        "        raise ValueError(\"La query deve essere una stringa non vuota.\")\n",
        "    if lang not in {\"it\", \"en\"}:\n",
        "        raise ValueError(\"Il parametro 'lang' deve essere 'it' oppure 'en'.\")\n",
        "\n",
        "    # Normalizzazione: unicode, trim, lower, spazi multipli\n",
        "    q = unicodedata.normalize(\"NFKC\", query).strip().lower()\n",
        "    q = \" \".join(q.split())\n",
        "    q = q.replace(\"'\", \"'\")\n",
        "\n",
        "    encoded_query = quote_plus(q)\n",
        "    base_url = f\"https://osha.europa.eu/{lang}/publications\"\n",
        "    return f\"{base_url}?search_api_fulltext={encoded_query}&sort_by={sort}\"\n",
        "\n",
        "\n",
        "def get_osha_publication_links(search_url: str, max_pages: int = 5) -> List[Dict[str, str]]:\n",
        "\n",
        "    # Estrae i link con anti-detection migliorato e delay adattivo\n",
        "\n",
        "    publications = []\n",
        "    seen_urls = set()  # Track duplicati\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.9',\n",
        "        'Accept-Encoding': 'gzip, deflate, br',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Upgrade-Insecure-Requests': '1',\n",
        "        'Sec-Fetch-Dest': 'document',\n",
        "        'Sec-Fetch-Mode': 'navigate',\n",
        "        'Sec-Fetch-Site': 'none',\n",
        "        'Cache-Control': 'max-age=0'\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ESTRAZIONE LINK PUBBLICAZIONI\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    print(f\"Search URL: {search_url}\")\n",
        "    print(f\"Max pagine: {max_pages}\\n\")\n",
        "\n",
        "    # Session per mantenere cookies\n",
        "    session = requests.Session()         # Mantiene cookies tra richieste (OSHA potrebbe usarli per tracking)\n",
        "    session.headers.update(headers)\n",
        "\n",
        "    for page_num in range(max_pages):\n",
        "        # URL con paginazione\n",
        "        if page_num == 0:\n",
        "            url = search_url\n",
        "        else:\n",
        "            separator = '&' if '?' in search_url else '?'\n",
        "            url = f\"{search_url}{separator}page={page_num}\"\n",
        "\n",
        "        print(f\"[Pagina {page_num + 1}/{max_pages}] Fetching: {url}\")\n",
        "\n",
        "        # Delay progressivo per evitare 403\n",
        "        if page_num > 0:\n",
        "            delay = 5 + (page_num * 2)  # 5, 7, 9, 11 secondi...\n",
        "            print(f\"  [~] Attesa {delay} secondi per evitare rate limit...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "        try:\n",
        "            response = session.get(url, timeout=30)\n",
        "\n",
        "            # Gestisci 403\n",
        "            if response.status_code == 403:\n",
        "                print(f\"  [!] 403 Forbidden - Server ha bloccato la richiesta\")\n",
        "                print(f\"  [INFO] Aumenta delay o riprova più tardi\")\n",
        "                print(f\"  [INFO] Pubblicazioni raccolte finora: {len(publications)}\")\n",
        "                break\n",
        "\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "            # DEBUG: Salva prima pagina\n",
        "            if page_num == 0:\n",
        "                with open('debug_osha_page.html', 'w', encoding='utf-8') as f:\n",
        "                    f.write(soup.prettify())\n",
        "\n",
        "            results_found = False\n",
        "            page_pubs = []\n",
        "\n",
        "            # METODO 1: article con classe node--type-publication\n",
        "            articles = soup.find_all('article', class_='node--type-publication')\n",
        "            if articles:\n",
        "                print(f\"  [+] Metodo 1: Trovati {len(articles)} articles\")\n",
        "                results_found = True\n",
        "\n",
        "                for article in articles:\n",
        "                    title_link = article.find('h2', class_='node__title')\n",
        "                    if not title_link:\n",
        "                        title_link = article.find('h2')\n",
        "\n",
        "                    if title_link:\n",
        "                        a_tag = title_link.find('a')\n",
        "                        if a_tag and a_tag.get('href'):\n",
        "                            href = a_tag['href']\n",
        "                            if href.startswith('/'):\n",
        "                                full_url = f\"https://osha.europa.eu{href}\"\n",
        "                            else:\n",
        "                                full_url = href\n",
        "\n",
        "                            # Skip duplicati\n",
        "                            if full_url not in seen_urls:\n",
        "                                title = a_tag.text.strip()\n",
        "                                page_pubs.append({\n",
        "                                    'title': title,\n",
        "                                    'url': full_url\n",
        "                                })\n",
        "                                seen_urls.add(full_url)\n",
        "\n",
        "            # METODO 2: Ricerca link diretti\n",
        "            if not results_found:\n",
        "                # Pattern più specifico: solo pubblicazioni vere\n",
        "                pattern = re.compile(r'/(en|it)/publications/[a-z0-9-]+$')\n",
        "                all_links = soup.find_all('a', href=pattern)\n",
        "\n",
        "                if all_links:\n",
        "                    print(f\"  [+] Metodo 2: Trovati {len(all_links)} link\")\n",
        "                    results_found = True\n",
        "\n",
        "                    for a_tag in all_links:\n",
        "                        href = a_tag['href']\n",
        "                        if href.startswith('/'):\n",
        "                            full_url = f\"https://osha.europa.eu{href}\"\n",
        "                        else:\n",
        "                            full_url = href\n",
        "\n",
        "                        # Skip duplicati e URL non validi\n",
        "                        if full_url not in seen_urls:\n",
        "                            # Prendi titolo dal link o cerca parent heading\n",
        "                            title = a_tag.text.strip()\n",
        "                            if not title or len(title) < 10:\n",
        "                                parent = a_tag.find_parent(['article', 'div'])\n",
        "                                if parent:\n",
        "                                    heading = parent.find(['h2', 'h3'])\n",
        "                                    if heading:\n",
        "                                        title = heading.text.strip()\n",
        "\n",
        "                            if title and len(title) >= 10:\n",
        "                                page_pubs.append({\n",
        "                                    'title': title,\n",
        "                                    'url': full_url\n",
        "                                })\n",
        "                                seen_urls.add(full_url)\n",
        "\n",
        "            if not results_found:\n",
        "                print(f\"  [!] Nessun risultato in pagina {page_num + 1} - fine\")\n",
        "                break\n",
        "\n",
        "            # Mostra trovati in questa pagina\n",
        "            print(f\"  [FOUND] {len(page_pubs)} nuove pubblicazioni:\")\n",
        "            for pub in page_pubs:\n",
        "                print(f\"    - {pub['title'][:75]}...\")\n",
        "                publications.append(pub)\n",
        "\n",
        "            # Se la pagina ha meno di 5 risultati, probabilmente è l'ultima\n",
        "            if len(page_pubs) < 5:\n",
        "                print(f\"  [INFO] Pagina con pochi risultati - probabilmente ultima pagina\")\n",
        "                break\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  [!] Errore su pagina {page_num + 1}: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\n[RESULT] Totale: {len(publications)} pubblicazioni uniche trovate\")\n",
        "    return publications\n"
      ],
      "metadata": {
        "id": "LiMVzZIh0_eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Processing Orchestrator\n",
        "\n",
        "Main pipeline that processes multiple OSHA publications sequentially. Handles link extraction, document scraping with Docling, individual and aggregate JSON export, error recovery, rate limiting, and aggregate statistics (total pages/tables/figures across all documents). Ensures efficient resource usage by reusing the Docling converter instance.\n"
      ],
      "metadata": {
        "id": "iTE69H3u2gKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_scrape_osha_publications(\n",
        "    search_url: str,\n",
        "    max_pages: int = 5,\n",
        "    max_documents: Optional[int] = None,\n",
        "    output_file: Optional[str] = None,\n",
        "    save_pdfs: bool = True,\n",
        "    delay_between_docs: int = 3\n",
        ") -> List[Dict[str, Any]]:\n",
        "\n",
        "    # Setup\n",
        "    ScraperConfig.setup_directories()\n",
        "    converter = initialize_docling_converter(enable_vlm=ScraperConfig.VLM_ENABLED)\n",
        "\n",
        "    # 1) Estrae tutti i link\n",
        "    publication_links = get_osha_publication_links(search_url, max_pages)\n",
        "\n",
        "    if not publication_links:\n",
        "        print(\"[WARNING] Nessuna pubblicazione trovata!\")\n",
        "        return []\n",
        "\n",
        "    # Limita numero documenti se richiesto\n",
        "    if max_documents:\n",
        "        publication_links = publication_links[:max_documents]\n",
        "        print(f\"\\n[INFO] Limitato a {max_documents} documenti\")\n",
        "\n",
        "    # 2) Scrape ogni pubblicazione\n",
        "    results = []\n",
        "    failed = []\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"INIZIO BATCH SCRAPING - {len(publication_links)} DOCUMENTI\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    for idx, pub in enumerate(publication_links, 1):\n",
        "        print(f\"\\n[{idx}/{len(publication_links)}] Processing: {pub['title'][:60]}...\")\n",
        "        print(f\"URL: {pub['url']}\")\n",
        "\n",
        "        try:\n",
        "            # Scrape con Docling\n",
        "            result = scrape_osha_publication(\n",
        "                publication_url=pub['url'],\n",
        "                converter=converter,\n",
        "                save_pdf=save_pdfs\n",
        "            )\n",
        "\n",
        "            if result['status'] == 'success':\n",
        "                results.append(result)\n",
        "\n",
        "                # Salva JSON individuale\n",
        "                json_path = save_to_json(result)\n",
        "                print(f\"  [+] Salvato: {json_path.name}\")\n",
        "            else:\n",
        "                failed.append({\n",
        "                    'url': pub['url'],\n",
        "                    'title': pub['title'],\n",
        "                    'error': result.get('error_message', 'Unknown error')\n",
        "                })\n",
        "                print(f\"  [!] Fallito: {result.get('error_message', 'Unknown')}\")\n",
        "\n",
        "            # Rate limiting - evita di sovraccaricare il server\n",
        "            if idx < len(publication_links):\n",
        "                print(f\"  [~] Pausa {delay_between_docs} secondi...\")\n",
        "                time.sleep(delay_between_docs)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [!] Eccezione: {e}\")\n",
        "            failed.append({\n",
        "                'url': pub['url'],\n",
        "                'title': pub['title'],\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "    # 3) Salva risultati aggregati (opzionale)\n",
        "    if output_file:\n",
        "        output_path = ScraperConfig.JSON_DIR / output_file\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"\\n[SAVED] Risultati aggregati in: {output_path}\")\n",
        "\n",
        "    # 4) Summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"BATCH SCRAPING COMPLETATO\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Successi: {len(results)}/{len(publication_links)}\")\n",
        "    print(f\"Falliti: {len(failed)}/{len(publication_links)}\")\n",
        "\n",
        "    if failed:\n",
        "        print(f\"\\nDocumenti falliti:\")\n",
        "        for fail in failed:\n",
        "            print(f\"  - {fail['title'][:60]}: {fail['error']}\")\n",
        "\n",
        "    # Statistiche aggregate\n",
        "    total_pages = sum(r['document_content']['num_pages']\n",
        "                     for r in results\n",
        "                     if r.get('document_content') and r['document_content'].get('num_pages') != 'N/A')\n",
        "    total_tables = sum(r['document_content']['num_tables']\n",
        "                      for r in results\n",
        "                      if r.get('document_content'))\n",
        "    total_figures = sum(r['document_content']['num_figures']\n",
        "                       for r in results\n",
        "                       if r.get('document_content'))\n",
        "\n",
        "    print(f\"\\nStatistiche aggregate:\")\n",
        "    print(f\"  - Pagine totali: {total_pages}\")\n",
        "    print(f\"  - Tabelle totali: {total_tables}\")\n",
        "    print(f\"  - Figure totali: {total_figures}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "-yD0EO2W1kgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interactive Batch Scraping Interface\n",
        "\n",
        "User-friendly CLI wrapper for batch scraping. Collects search parameters through prompts (keyword, language, page/document limits), validates inputs, shows confirmation dialog, and executes the full pipeline. Auto-generates descriptive output filenames. Perfect for exploratory scraping sessions in notebooks.\n"
      ],
      "metadata": {
        "id": "wGWSf9CT2plW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_batch_scraping():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OSHA BATCH SCRAPER CON DOCLING\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Input query\n",
        "    query = input(\"Inserisci la parola chiave o frase da cercare: \").strip()\n",
        "    if not query:\n",
        "        print(\"[ERROR] Query vuota!\")\n",
        "        return\n",
        "\n",
        "    # Input lingua\n",
        "    lang = input(\"Lingua ('it' per italiano, 'en' per inglese): \").strip().lower()\n",
        "    while lang not in {\"it\", \"en\"}:\n",
        "        print(\"[ERROR] Lingua non valida!\")\n",
        "        lang = input(\"Lingua ('it' o 'en'): \").strip().lower()\n",
        "\n",
        "    # Input numero pagine\n",
        "    try:\n",
        "        max_pages = int(input(\"Numero massimo di pagine di risultati (default 5): \").strip() or \"5\")\n",
        "    except ValueError:\n",
        "        max_pages = 5\n",
        "        print(f\"[INFO] Usando default: {max_pages} pagine\")\n",
        "\n",
        "    # Input limite documenti\n",
        "    max_docs_input = input(\"Limite documenti da processare (Enter = tutti): \").strip()\n",
        "    max_docs = int(max_docs_input) if max_docs_input else None\n",
        "\n",
        "    # Costruisce URL\n",
        "    search_url = build_osha_search_url(query, lang=lang)\n",
        "    print(f\"\\n[INFO] URL di ricerca: {search_url}\\n\")\n",
        "\n",
        "    # Conferma\n",
        "    confirm = input(\"Procedere con lo scraping? (y/n): \").strip().lower()\n",
        "    if confirm != 'y':\n",
        "        print(\"[CANCELLED] Operazione annullata\")\n",
        "        return\n",
        "\n",
        "    # Output filename\n",
        "    safe_query = re.sub(r'[^\\w\\-_.]', '_', query[:30])\n",
        "    output_file = f\"batch_{safe_query}_{lang}.json\"\n",
        "\n",
        "    # Esegui batch scraping\n",
        "    results = batch_scrape_osha_publications(\n",
        "        search_url=search_url,\n",
        "        max_pages=max_pages,\n",
        "        max_documents=max_docs,\n",
        "        output_file=output_file,\n",
        "        save_pdfs=True,\n",
        "        delay_between_docs=3\n",
        "    )\n",
        "\n",
        "    print(f\"\\n[SUCCESS] Batch scraping completato!\")\n",
        "    print(f\"Risultati salvati in: {ScraperConfig.JSON_DIR}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Kp-15jTL1Xpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Script Entry Point\n",
        "\n",
        "Launches the interactive scraping interface when the script runs standalone."
      ],
      "metadata": {
        "id": "Y76S9tDt27qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Modalità interattiva - chiede input all'utente\n",
        "    interactive_batch_scraping()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hybhp_6h1RvE",
        "outputId": "bb74f42b-c334-4e7b-e993-dee55c06ff92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "OSHA BATCH SCRAPER CON DOCLING\n",
            "================================================================================\n",
            "\n",
            "Inserisci la parola chiave o frase da cercare: work safety\n",
            "Lingua ('it' per italiano, 'en' per inglese): en\n",
            "Numero massimo di pagine di risultati (default 5): 2\n",
            "Limite documenti da processare (Enter = tutti): \n",
            "\n",
            "[INFO] URL di ricerca: https://osha.europa.eu/en/publications?search_api_fulltext=work+safety&sort_by=field_publication_date\n",
            "\n",
            "Procedere con lo scraping? (y/n): y\n",
            "VLM abilitato: HuggingFaceTB/SmolVLM-256M-Instruct\n",
            "\n",
            "================================================================================\n",
            "ESTRAZIONE LINK PUBBLICAZIONI\n",
            "================================================================================\n",
            "\n",
            "Search URL: https://osha.europa.eu/en/publications?search_api_fulltext=work+safety&sort_by=field_publication_date\n",
            "Max pagine: 2\n",
            "\n",
            "[Pagina 1/2] Fetching: https://osha.europa.eu/en/publications?search_api_fulltext=work+safety&sort_by=field_publication_date\n",
            "  [DEBUG] HTML salvato in debug_osha_page.html\n",
            "  [+] Metodo 3: Trovati 24 link (ricerca generica)\n",
            "    - Musculoskeletal health and risk factors in the HeSCare sector – a review of exis...\n",
            "    - OSH Pulse 2025: Occupational safety and health in the era of climate and digital...\n",
            "    - Strategies and legislation on psychosocial risks in six European countries...\n",
            "    - Digital platform work and occupational safety and health: overview of regulation...\n",
            "    - Summary - Digital platform work and occupational safety and health: overview of ...\n",
            "    - Occupational safety and health risks of remote programming work organised throug...\n",
            "    - Occupational safety and health risks of parcel delivery work organised through d...\n",
            "    - Occupational safety and health risks of online content review work provided thro...\n",
            "[Pagina 2/2] Fetching: https://osha.europa.eu/en/publications?search_api_fulltext=work+safety&sort_by=field_publication_date&page=1\n",
            "  [!] Errore su pagina 2: 403 Client Error: Forbidden for url: https://osha.europa.eu/en/publications?search_api_fulltext=work+safety&sort_by=field_publication_date&page=1\n",
            "\n",
            "[RESULT] Totale: 8 pubblicazioni trovate\n",
            "\n",
            "================================================================================\n",
            "INIZIO BATCH SCRAPING - 8 DOCUMENTI\n",
            "================================================================================\n",
            "\n",
            "\n",
            "[1/8] Processing: Musculoskeletal health and risk factors in the HeSCare secto...\n",
            "URL: https://osha.europa.eu/en/publications/musculoskeletal-health-and-risk-factors-hescare-sector-review-existing-information\n",
            "\n",
            "================================================================================\n",
            "Scraping pubblicazione: https://osha.europa.eu/en/publications/musculoskeletal-health-and-risk-factors-hescare-sector-review-existing-information\n",
            "================================================================================\n",
            "\n",
            "Estrazione metadati dalla pagina web...\n",
            "Titolo: Musculoskeletal health and risk factors in the HeSCare sector – a review of existing information\n",
            "Data: 03/10/2025\n",
            "Keywords: Health and social care, Musculoskeletal disorders\n",
            "PDF scaricato: Musculoskeletal_health_and_risk_factors_in_the_HeS.pdf\n",
            "Processing con Docling: Musculoskeletal_health_and_risk_factors_in_the_HeS.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:51,937 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:51,946 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/det/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:54,666 [RapidOCR] download_file.py:82: Download size: 13.83MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:54,879 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:54,883 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:55,308 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:55,309 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/cls/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:56,657 [RapidOCR] download_file.py:82: Download size: 0.56MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:56,669 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:56,670 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:56,748 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:56,750 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:58,620 [RapidOCR] download_file.py:82: Download size: 25.67MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:58,893 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 13:59:58,894 [RapidOCR] torch.py:54: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 14:01:19,320 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/resources/fonts/FZYTK.TTF\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 14:01:21,072 [RapidOCR] download_file.py:82: Download size: 3.09MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-10-23 14:01:21,696 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/FZYTK.TTF\u001b[0m\n",
            "WARNING:docling_core.types.doc.document:Parameter `strict_text` has been deprecated and will be ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nessun elemento trovato in 1213 items, fallback markdown\n",
            "Struttura da Markdown: 212 headings, 15 tabelle, 17 figure\n",
            "[DEBUG] Usando iterate_items()\n",
            "  [+] Salvata: figure_1.png\n",
            "  [+] Salvata: figure_2.png\n",
            "  [+] Salvata: figure_3.png\n",
            "  [+] Salvata: figure_4.png\n",
            "  [+] Salvata: figure_5.png\n",
            "  [+] Salvata: figure_6.png\n",
            "  [+] Salvata: figure_7.png\n",
            "  [+] Salvata: figure_8.png\n",
            "  [+] Salvata: figure_9.png\n",
            "  [+] Salvata: figure_10.png\n",
            "  [+] Salvata: figure_11.png\n",
            "  [+] Salvata: figure_12.png\n",
            "  [+] Salvata: figure_13.png\n",
            "  [+] Salvata: figure_14.png\n",
            "  [+] Salvata: figure_15.png\n",
            "  [+] Salvata: figure_16.png\n",
            "  [+] Salvata: figure_17.png\n",
            "[RESULT] 17 immagini esportate in: osha_scraped_data/images/Musculoskeletal_health_and_risk_factors_in_the_HeS\n",
            " Tabella salvata: table_0.txt\n",
            " Tabella salvata: table_1.txt\n",
            " Tabella salvata: table_2.txt\n",
            " Tabella salvata: table_3.txt\n",
            " Tabella salvata: table_4.txt\n",
            " Tabella salvata: table_5.txt\n",
            " Tabella salvata: table_6.txt\n",
            " Tabella salvata: table_7.txt\n",
            " Tabella salvata: table_8.txt\n",
            " Tabella salvata: table_9.txt\n",
            " Tabella salvata: table_10.txt\n",
            " Tabella salvata: table_11.txt\n",
            " Tabella salvata: table_12.txt\n",
            " Tabella salvata: table_13.txt\n",
            " Tabella salvata: table_14.txt\n",
            "15 tabelle esportate in: osha_scraped_data/tables/Musculoskeletal_health_and_risk_factors_in_the_HeS\n",
            "Docling processing completato:\n",
            "  - 111 pagine\n",
            "  - 15 tabelle\n",
            "  - 17 figure\n",
            "  - 212 headings\n",
            "  - 17 immagini esportate\n",
            "  - 15 tabelle esportate\n",
            "\n",
            " Scraping completato con successo!\n",
            "Dati salvati in: osha_scraped_data/json/Musculoskeletal_health_and_risk_factors_in_the_HeS_20251023_140529.json\n",
            "  [+] Salvato: Musculoskeletal_health_and_risk_factors_in_the_HeS_20251023_140529.json\n",
            "  [~] Pausa 3 secondi...\n",
            "\n",
            "[2/8] Processing: OSH Pulse 2025: Occupational safety and health in the era of...\n",
            "URL: https://osha.europa.eu/en/publications/osh-pulse-2025-occupational-safety-and-health-era-climate-and-digital-change\n",
            "\n",
            "================================================================================\n",
            "Scraping pubblicazione: https://osha.europa.eu/en/publications/osh-pulse-2025-occupational-safety-and-health-era-climate-and-digital-change\n",
            "================================================================================\n",
            "\n",
            "Estrazione metadati dalla pagina web...\n",
            "Titolo: OSH Pulse 2025: Occupational safety and health in the era of climate and digital change\n",
            "Data: 23/09/2025\n",
            "Keywords: Digitalisation, Emerging risks, Psychosocial risks and mental health\n",
            "PDF scaricato: OSH_Pulse_2025__Occupational_safety_and_health_in_.pdf\n",
            "Processing con Docling: OSH_Pulse_2025__Occupational_safety_and_health_in_.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:docling_core.types.doc.document:Parameter `strict_text` has been deprecated and will be ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nessun elemento trovato in 637 items, fallback markdown\n",
            "Struttura da Markdown: 74 headings, 45 tabelle, 61 figure\n",
            "[DEBUG] Usando iterate_items()\n",
            "  [+] Salvata: figure_1.png\n",
            "  [+] Salvata: figure_2.png\n",
            "  [+] Salvata: figure_3.png\n",
            "  [+] Salvata: figure_4.png\n",
            "  [+] Salvata: figure_5.png\n",
            "  [+] Salvata: figure_6.png\n",
            "  [+] Salvata: figure_7.png\n",
            "  [+] Salvata: figure_8.png\n",
            "  [+] Salvata: figure_9.png\n",
            "  [+] Salvata: figure_10.png\n",
            "  [+] Salvata: figure_11.png\n",
            "  [+] Salvata: figure_12.png\n",
            "  [+] Salvata: figure_13.png\n",
            "  [+] Salvata: figure_14.png\n",
            "  [+] Salvata: figure_15.png\n",
            "  [+] Salvata: figure_16.png\n",
            "  [+] Salvata: figure_17.png\n",
            "  [+] Salvata: figure_18.png\n",
            "  [+] Salvata: figure_19.png\n",
            "  [+] Salvata: figure_20.png\n",
            "  [+] Salvata: figure_21.png\n",
            "  [+] Salvata: figure_22.png\n",
            "  [+] Salvata: figure_23.png\n",
            "  [+] Salvata: figure_24.png\n",
            "  [+] Salvata: figure_25.png\n",
            "  [+] Salvata: figure_26.png\n",
            "  [+] Salvata: figure_27.png\n",
            "  [+] Salvata: figure_28.png\n",
            "  [+] Salvata: figure_29.png\n",
            "  [+] Salvata: figure_30.png\n",
            "  [+] Salvata: figure_31.png\n",
            "  [+] Salvata: figure_32.png\n",
            "  [+] Salvata: figure_33.png\n",
            "  [+] Salvata: figure_34.png\n",
            "  [+] Salvata: figure_35.png\n",
            "  [+] Salvata: figure_36.png\n",
            "  [+] Salvata: figure_37.png\n",
            "  [+] Salvata: figure_38.png\n",
            "  [+] Salvata: figure_39.png\n",
            "  [+] Salvata: figure_40.png\n",
            "  [+] Salvata: figure_41.png\n",
            "  [+] Salvata: figure_42.png\n",
            "  [+] Salvata: figure_43.png\n",
            "  [+] Salvata: figure_44.png\n",
            "  [+] Salvata: figure_45.png\n",
            "  [+] Salvata: figure_46.png\n",
            "  [+] Salvata: figure_47.png\n",
            "  [+] Salvata: figure_48.png\n",
            "  [+] Salvata: figure_49.png\n",
            "  [+] Salvata: figure_50.png\n",
            "  [+] Salvata: figure_51.png\n",
            "  [+] Salvata: figure_52.png\n",
            "  [+] Salvata: figure_53.png\n",
            "  [+] Salvata: figure_54.png\n",
            "  [+] Salvata: figure_55.png\n",
            "  [+] Salvata: figure_56.png\n",
            "  [+] Salvata: figure_57.png\n",
            "  [+] Salvata: figure_58.png\n",
            "  [+] Salvata: figure_59.png\n",
            "  [+] Salvata: figure_60.png\n",
            "  [+] Salvata: figure_61.png\n",
            "[RESULT] 61 immagini esportate in: osha_scraped_data/images/OSH_Pulse_2025__Occupational_safety_and_health_in_\n",
            " Tabella salvata: table_0.txt\n",
            " Tabella salvata: table_1.txt\n",
            " Tabella salvata: table_2.txt\n",
            " Tabella salvata: table_3.txt\n",
            " Tabella salvata: table_4.txt\n",
            " Tabella salvata: table_5.txt\n",
            " Tabella salvata: table_6.txt\n",
            " Tabella salvata: table_7.txt\n",
            " Tabella salvata: table_8.txt\n",
            " Tabella salvata: table_9.txt\n",
            " Tabella salvata: table_10.txt\n",
            " Tabella salvata: table_11.txt\n",
            " Tabella salvata: table_12.txt\n",
            " Tabella salvata: table_13.txt\n",
            " Tabella salvata: table_14.txt\n",
            " Tabella salvata: table_15.txt\n",
            " Tabella salvata: table_16.txt\n",
            " Tabella salvata: table_17.txt\n",
            " Tabella salvata: table_18.txt\n",
            " Tabella salvata: table_19.txt\n",
            " Tabella salvata: table_20.txt\n",
            " Tabella salvata: table_21.txt\n",
            " Tabella salvata: table_22.txt\n",
            " Tabella salvata: table_23.txt\n",
            " Tabella salvata: table_24.txt\n",
            " Tabella salvata: table_25.txt\n",
            " Tabella salvata: table_26.txt\n",
            " Tabella salvata: table_27.txt\n",
            " Tabella salvata: table_28.txt\n",
            " Tabella salvata: table_29.txt\n",
            " Tabella salvata: table_30.txt\n",
            " Tabella salvata: table_31.txt\n",
            " Tabella salvata: table_32.txt\n",
            " Tabella salvata: table_33.txt\n",
            " Tabella salvata: table_34.txt\n",
            " Tabella salvata: table_35.txt\n",
            " Tabella salvata: table_36.txt\n",
            " Tabella salvata: table_37.txt\n",
            " Tabella salvata: table_38.txt\n",
            " Tabella salvata: table_39.txt\n",
            " Tabella salvata: table_40.txt\n",
            " Tabella salvata: table_41.txt\n",
            " Tabella salvata: table_42.txt\n",
            " Tabella salvata: table_43.txt\n",
            " Tabella salvata: table_44.txt\n",
            "45 tabelle esportate in: osha_scraped_data/tables/OSH_Pulse_2025__Occupational_safety_and_health_in_\n",
            "Docling processing completato:\n",
            "  - 65 pagine\n",
            "  - 45 tabelle\n",
            "  - 61 figure\n",
            "  - 74 headings\n",
            "  - 61 immagini esportate\n",
            "  - 45 tabelle esportate\n",
            "\n",
            " Scraping completato con successo!\n",
            "Dati salvati in: osha_scraped_data/json/OSH_Pulse_2025__Occupational_safety_and_health_in__20251023_143117.json\n",
            "  [+] Salvato: OSH_Pulse_2025__Occupational_safety_and_health_in__20251023_143117.json\n",
            "  [~] Pausa 3 secondi...\n",
            "\n",
            "[3/8] Processing: Strategies and legislation on psychosocial risks in six Euro...\n",
            "URL: https://osha.europa.eu/en/publications/strategies-and-legislation-psychosocial-risks-six-european-countries\n",
            "\n",
            "================================================================================\n",
            "Scraping pubblicazione: https://osha.europa.eu/en/publications/strategies-and-legislation-psychosocial-risks-six-european-countries\n",
            "================================================================================\n",
            "\n",
            "Estrazione metadati dalla pagina web...\n",
            "Titolo: Strategies and legislation on psychosocial risks in six European countries\n",
            "Data: 28/05/2025\n",
            "Keywords: Policy Makers, Psychosocial risks and mental health\n",
            "PDF scaricato: Strategies_and_legislation_on_psychosocial_risks_i.pdf\n",
            "Processing con Docling: Strategies_and_legislation_on_psychosocial_risks_i.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:docling_core.types.doc.document:Parameter `strict_text` has been deprecated and will be ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nessun elemento trovato in 267 items, fallback markdown\n",
            "Struttura da Markdown: 33 headings, 0 tabelle, 2 figure\n",
            "[DEBUG] Usando iterate_items()\n",
            "  [+] Salvata: figure_1.png\n",
            "  [+] Salvata: figure_2.png\n",
            "[RESULT] 2 immagini esportate in: osha_scraped_data/images/Strategies_and_legislation_on_psychosocial_risks_i\n",
            "Docling processing completato:\n",
            "  - 27 pagine\n",
            "  - 0 tabelle\n",
            "  - 2 figure\n",
            "  - 33 headings\n",
            "  - 2 immagini esportate\n",
            "  - 0 tabelle esportate\n",
            "\n",
            " Scraping completato con successo!\n",
            "Dati salvati in: osha_scraped_data/json/Strategies_and_legislation_on_psychosocial_risks_i_20251023_151011.json\n",
            "  [+] Salvato: Strategies_and_legislation_on_psychosocial_risks_i_20251023_151011.json\n",
            "  [~] Pausa 3 secondi...\n",
            "\n",
            "[4/8] Processing: Digital platform work and occupational safety and health: ov...\n",
            "URL: https://osha.europa.eu/en/publications/digital-platform-work-and-occupational-safety-and-health-overview-regulation-policies-practices-and-research\n",
            "\n",
            "================================================================================\n",
            "Scraping pubblicazione: https://osha.europa.eu/en/publications/digital-platform-work-and-occupational-safety-and-health-overview-regulation-policies-practices-and-research\n",
            "================================================================================\n",
            "\n",
            "Estrazione metadati dalla pagina web...\n",
            "Titolo: Digital platform work and occupational safety and health: overview of regulation, policies, practices and research\n",
            "Data: 18/03/2022\n",
            "Keywords: Digitalisation, Emerging risks, Psychosocial risks and mental health\n",
            "PDF scaricato: Digital_platform_work_and_occupational_safety_and_.pdf\n",
            "Processing con Docling: Digital_platform_work_and_occupational_safety_and_.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:docling_core.types.doc.document:Parameter `strict_text` has been deprecated and will be ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nessun elemento trovato in 570 items, fallback markdown\n",
            "Struttura da Markdown: 78 headings, 7 tabelle, 7 figure\n",
            "[DEBUG] Usando iterate_items()\n",
            "  [+] Salvata: figure_1.png\n",
            "  [+] Salvata: figure_2.png\n",
            "  [+] Salvata: figure_3.png\n",
            "  [+] Salvata: figure_4.png\n",
            "  [+] Salvata: figure_5.png\n",
            "  [+] Salvata: figure_6.png\n",
            "  [+] Salvata: figure_7.png\n",
            "[RESULT] 7 immagini esportate in: osha_scraped_data/images/Digital_platform_work_and_occupational_safety_and_\n",
            " Tabella salvata: table_0.txt\n",
            " Tabella salvata: table_1.txt\n",
            " Tabella salvata: table_2.txt\n",
            " Tabella salvata: table_3.txt\n",
            " Tabella salvata: table_4.txt\n",
            " Tabella salvata: table_5.txt\n",
            " Tabella salvata: table_6.txt\n",
            "7 tabelle esportate in: osha_scraped_data/tables/Digital_platform_work_and_occupational_safety_and_\n",
            "Docling processing completato:\n",
            "  - 56 pagine\n",
            "  - 7 tabelle\n",
            "  - 7 figure\n",
            "  - 78 headings\n",
            "  - 7 immagini esportate\n",
            "  - 7 tabelle esportate\n",
            "\n",
            " Scraping completato con successo!\n",
            "Dati salvati in: osha_scraped_data/json/Digital_platform_work_and_occupational_safety_and__20251023_151249.json\n",
            "  [+] Salvato: Digital_platform_work_and_occupational_safety_and__20251023_151249.json\n",
            "  [~] Pausa 3 secondi...\n",
            "\n",
            "[5/8] Processing: Summary - Digital platform work and occupational safety and ...\n",
            "URL: https://osha.europa.eu/en/publications/summary-digital-platform-work-and-occupational-safety-and-health-overview-regulation-policies-practices-and-research\n",
            "\n",
            "================================================================================\n",
            "Scraping pubblicazione: https://osha.europa.eu/en/publications/summary-digital-platform-work-and-occupational-safety-and-health-overview-regulation-policies-practices-and-research\n",
            "================================================================================\n",
            "\n",
            "Estrazione metadati dalla pagina web...\n",
            "Titolo: Summary - Digital platform work and occupational safety and health: overview of regulation, policies, practices and research\n",
            "Data: 18/03/2022\n",
            "Keywords: Digitalisation, Emerging risks, Psychosocial risks and mental health\n",
            "PDF scaricato: Summary_-_Digital_platform_work_and_occupational_s.pdf\n",
            "Processing con Docling: Summary_-_Digital_platform_work_and_occupational_s.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:docling_core.types.doc.document:Parameter `strict_text` has been deprecated and will be ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nessun elemento trovato in 223 items, fallback markdown\n",
            "Struttura da Markdown: 28 headings, 1 tabelle, 6 figure\n",
            "[DEBUG] Usando iterate_items()\n",
            "  [+] Salvata: figure_1.png\n",
            "  [+] Salvata: figure_2.png\n",
            "  [+] Salvata: figure_3.png\n",
            "  [+] Salvata: figure_4.png\n",
            "  [+] Salvata: figure_5.png\n",
            "  [+] Salvata: figure_6.png\n",
            "[RESULT] 6 immagini esportate in: osha_scraped_data/images/Summary_-_Digital_platform_work_and_occupational_s\n",
            " Tabella salvata: table_0.txt\n",
            "1 tabelle esportate in: osha_scraped_data/tables/Summary_-_Digital_platform_work_and_occupational_s\n",
            "Docling processing completato:\n",
            "  - 25 pagine\n",
            "  - 1 tabelle\n",
            "  - 6 figure\n",
            "  - 28 headings\n",
            "  - 6 immagini esportate\n",
            "  - 1 tabelle esportate\n",
            "\n",
            " Scraping completato con successo!\n",
            "Dati salvati in: osha_scraped_data/json/Summary_-_Digital_platform_work_and_occupational_s_20251023_151422.json\n",
            "  [+] Salvato: Summary_-_Digital_platform_work_and_occupational_s_20251023_151422.json\n",
            "  [~] Pausa 3 secondi...\n",
            "\n",
            "[6/8] Processing: Occupational safety and health risks of remote programming w...\n",
            "URL: https://osha.europa.eu/en/publications/occupational-safety-and-health-risks-remote-programming-work-organised-through-digital-labour-platforms\n",
            "\n",
            "================================================================================\n",
            "Scraping pubblicazione: https://osha.europa.eu/en/publications/occupational-safety-and-health-risks-remote-programming-work-organised-through-digital-labour-platforms\n",
            "================================================================================\n",
            "\n",
            "Estrazione metadati dalla pagina web...\n",
            "Titolo: Occupational safety and health risks of remote programming work organised through digital labour platforms\n",
            "Data: 08/03/2022\n",
            "Keywords: Digitalisation, Emerging risks, Musculoskeletal disorders, Psychosocial risks and mental health\n",
            "PDF scaricato: Occupational_safety_and_health_risks_of_remote_pro.pdf\n",
            "Processing con Docling: Occupational_safety_and_health_risks_of_remote_pro.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:docling_core.types.doc.document:Parameter `strict_text` has been deprecated and will be ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nessun elemento trovato in 119 items, fallback markdown\n",
            "Struttura da Markdown: 24 headings, 1 tabelle, 3 figure\n",
            "[DEBUG] Usando iterate_items()\n",
            "  [+] Salvata: figure_1.png\n",
            "  [+] Salvata: figure_2.png\n",
            "  [+] Salvata: figure_3.png\n",
            "[RESULT] 3 immagini esportate in: osha_scraped_data/images/Occupational_safety_and_health_risks_of_remote_pro\n",
            " Tabella salvata: table_0.txt\n",
            "1 tabelle esportate in: osha_scraped_data/tables/Occupational_safety_and_health_risks_of_remote_pro\n",
            "Docling processing completato:\n",
            "  - 13 pagine\n",
            "  - 1 tabelle\n",
            "  - 3 figure\n",
            "  - 24 headings\n",
            "  - 3 immagini esportate\n",
            "  - 1 tabelle esportate\n",
            "\n",
            " Scraping completato con successo!\n",
            "Dati salvati in: osha_scraped_data/json/Occupational_safety_and_health_risks_of_remote_pro_20251023_153202.json\n",
            "  [+] Salvato: Occupational_safety_and_health_risks_of_remote_pro_20251023_153202.json\n",
            "  [~] Pausa 3 secondi...\n",
            "\n",
            "[7/8] Processing: Occupational safety and health risks of parcel delivery work...\n",
            "URL: https://osha.europa.eu/en/publications/occupational-safety-and-health-risks-parcel-delivery-work-organised-through-digital-labour-platforms\n",
            "\n",
            "================================================================================\n",
            "Scraping pubblicazione: https://osha.europa.eu/en/publications/occupational-safety-and-health-risks-parcel-delivery-work-organised-through-digital-labour-platforms\n",
            "================================================================================\n",
            "\n",
            "Estrazione metadati dalla pagina web...\n",
            "Titolo: Occupational safety and health risks of parcel delivery work organised through digital labour platforms\n",
            "Data: 08/03/2022\n",
            "Keywords: Digitalisation, Emerging risks, Psychosocial risks and mental health, Road Transport\n",
            "PDF scaricato: Occupational_safety_and_health_risks_of_parcel_del.pdf\n",
            "Processing con Docling: Occupational_safety_and_health_risks_of_parcel_del.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:docling_core.types.doc.document:Parameter `strict_text` has been deprecated and will be ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nessun elemento trovato in 138 items, fallback markdown\n",
            "Struttura da Markdown: 14 headings, 0 tabelle, 2 figure\n",
            "[DEBUG] Usando iterate_items()\n",
            "  [+] Salvata: figure_1.png\n",
            "  [+] Salvata: figure_2.png\n",
            "[RESULT] 2 immagini esportate in: osha_scraped_data/images/Occupational_safety_and_health_risks_of_parcel_del\n",
            "Docling processing completato:\n",
            "  - 14 pagine\n",
            "  - 0 tabelle\n",
            "  - 2 figure\n",
            "  - 14 headings\n",
            "  - 2 immagini esportate\n",
            "  - 0 tabelle esportate\n",
            "\n",
            " Scraping completato con successo!\n",
            "Dati salvati in: osha_scraped_data/json/Occupational_safety_and_health_risks_of_parcel_del_20251023_155401.json\n",
            "  [+] Salvato: Occupational_safety_and_health_risks_of_parcel_del_20251023_155401.json\n",
            "  [~] Pausa 3 secondi...\n",
            "\n",
            "[8/8] Processing: Occupational safety and health risks of online content revie...\n",
            "URL: https://osha.europa.eu/en/publications/occupational-safety-and-health-risks-online-content-review-work-provided-through-digital-labour-platforms\n",
            "\n",
            "================================================================================\n",
            "Scraping pubblicazione: https://osha.europa.eu/en/publications/occupational-safety-and-health-risks-online-content-review-work-provided-through-digital-labour-platforms\n",
            "================================================================================\n",
            "\n",
            "Estrazione metadati dalla pagina web...\n",
            "Titolo: Occupational safety and health risks of online content review work provided through digital labour platforms\n",
            "Data: 08/03/2022\n",
            "Keywords: Digitalisation, Emerging risks, Psychosocial risks and mental health\n",
            "PDF scaricato: Occupational_safety_and_health_risks_of_online_con.pdf\n",
            "Processing con Docling: Occupational_safety_and_health_risks_of_online_con.pdf...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incremental Scraping - Process Only New Documents\n",
        "\n",
        "Smart differential update system that compares search results against existing JSON archives, identifies new publications, and processes only those not previously scraped. Supports check-only mode for dry-run verification. Safely appends new data while preserving existing records. Returns detailed statistics (existing/new/failed counts). Interactive interface included for periodic archive updates with minimal overhead.\n"
      ],
      "metadata": {
        "id": "c9OmlRwO3IeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def incremental_batch_scraping(\n",
        "    search_url: str,\n",
        "    output_file: str,\n",
        "    max_pages: int = 5,\n",
        "    max_new_documents: Optional[int] = None,\n",
        "    save_pdfs: bool = True,\n",
        "    check_only: bool = False,\n",
        "    delay_between_docs: int = 3\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    output_path = ScraperConfig.JSON_DIR / output_file\n",
        "\n",
        "    # 1) Carica dati esistenti\n",
        "    existing_data = []\n",
        "    existing_urls = set()\n",
        "\n",
        "    if output_path.exists():\n",
        "        try:\n",
        "            with open(output_path, 'r', encoding='utf-8') as f:\n",
        "                existing_data = json.load(f)\n",
        "\n",
        "            # Estrae tutti gli URL già processati\n",
        "            for item in existing_data:\n",
        "                # Supporta diversi formati di URL storage\n",
        "                url = (item.get('scraping_metadata', {}).get('url') or\n",
        "                       item.get('web_metadata', {}).get('pdf_url') or\n",
        "                       item.get('url'))\n",
        "                if url:\n",
        "                    existing_urls.add(url)\n",
        "\n",
        "            print(f\"\\n[INFO] Caricato file esistente: {output_path}\")\n",
        "            print(f\"[INFO] Documenti già processati: {len(existing_urls)}\")\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"[ERROR] File JSON corrotto: {e}\")\n",
        "            print(f\"[INFO] Procedo come se il file non esistesse\")\n",
        "    else:\n",
        "        print(f\"\\n[INFO] File {output_file} non trovato - creazione nuovo archivio\")\n",
        "\n",
        "    # 2) Setup converter\n",
        "    ScraperConfig.setup_directories()\n",
        "    converter = initialize_docling_converter(enable_vlm=ScraperConfig.VLM_ENABLED)\n",
        "\n",
        "    # 3) Ottieni link pubblicazioni\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"CONTROLLO NUOVE PUBBLICAZIONI\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    all_links = get_osha_publication_links(search_url, max_pages)\n",
        "\n",
        "    if not all_links:\n",
        "        print(\"[WARNING] Nessuna pubblicazione trovata nella ricerca\")\n",
        "        return {\n",
        "            'existing': len(existing_data),\n",
        "            'new_found': 0,\n",
        "            'new_processed': 0,\n",
        "            'failed': 0\n",
        "        }\n",
        "\n",
        "    # 4) Filtra link nuovi\n",
        "    new_links = [link for link in all_links if link['url'] not in existing_urls]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ANALISI PUBBLICAZIONI\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Totale trovate: {len(all_links)}\")\n",
        "    print(f\"Già esistenti: {len(all_links) - len(new_links)}\")\n",
        "    print(f\"Nuove da processare: {len(new_links)}\")\n",
        "\n",
        "    if not new_links:\n",
        "        print(\"\\n[SUCCESS] Archivio già aggiornato - nessun nuovo documento!\")\n",
        "        return {\n",
        "            'existing': len(existing_data),\n",
        "            'new_found': 0,\n",
        "            'new_processed': 0,\n",
        "            'failed': 0\n",
        "        }\n",
        "\n",
        "    # Mostra lista nuovi documenti\n",
        "    print(f\"\\n[NEW] Pubblicazioni nuove trovate:\")\n",
        "    for idx, link in enumerate(new_links, 1):\n",
        "        print(f\"  {idx}. {link['title'][:70]}...\")\n",
        "        print(f\"     {link['url']}\")\n",
        "\n",
        "    # 5) Check-only mode\n",
        "    if check_only:\n",
        "        print(f\"\\n[CHECK-ONLY] Modalità verifica - nessun processing eseguito\")\n",
        "        return {\n",
        "            'existing': len(existing_data),\n",
        "            'new_found': len(new_links),\n",
        "            'new_processed': 0,\n",
        "            'failed': 0\n",
        "        }\n",
        "\n",
        "    # 6) Conferma processing\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    confirm = input(f\"Processare {len(new_links)} nuovi documenti? (y/n): \").strip().lower()\n",
        "    if confirm != 'y':\n",
        "        print(\"[CANCELLED] Operazione annullata\")\n",
        "        return {\n",
        "            'existing': len(existing_data),\n",
        "            'new_found': len(new_links),\n",
        "            'new_processed': 0,\n",
        "            'failed': 0\n",
        "        }\n",
        "\n",
        "    # Limita numero se richiesto\n",
        "    if max_new_documents:\n",
        "        new_links = new_links[:max_new_documents]\n",
        "        print(f\"[INFO] Limitato a {max_new_documents} documenti\")\n",
        "\n",
        "    # 7) Processa nuovi documenti\n",
        "    new_results = []\n",
        "    failed = []\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROCESSING NUOVI DOCUMENTI - {len(new_links)}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    for idx, link in enumerate(new_links, 1):\n",
        "        print(f\"\\n[{idx}/{len(new_links)}] Processing: {link['title'][:60]}...\")\n",
        "\n",
        "        try:\n",
        "            result = scrape_osha_publication(\n",
        "                publication_url=link['url'],\n",
        "                converter=converter,\n",
        "                save_pdf=save_pdfs\n",
        "            )\n",
        "\n",
        "            if result['status'] == 'success':\n",
        "                new_results.append(result)\n",
        "\n",
        "                # Salva JSON individuale\n",
        "                json_path = save_to_json(result)\n",
        "                print(f\"  [+] Salvato: {json_path.name}\")\n",
        "            else:\n",
        "                failed.append({\n",
        "                    'url': link['url'],\n",
        "                    'title': link['title'],\n",
        "                    'error': result.get('error_message', 'Unknown')\n",
        "                })\n",
        "                print(f\"  [!] Fallito: {result.get('error_message')}\")\n",
        "\n",
        "            # Rate limiting\n",
        "            if idx < len(new_links):\n",
        "                print(f\"  [~] Pausa {delay_between_docs} secondi...\")\n",
        "                time.sleep(delay_between_docs)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [!] Eccezione: {e}\")\n",
        "            failed.append({\n",
        "                'url': link['url'],\n",
        "                'title': link['title'],\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "    # 8) Aggiorna file aggregato\n",
        "    if new_results:\n",
        "        updated_data = existing_data + new_results\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(updated_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"\\n[SAVED] File aggregato aggiornato: {output_path}\")\n",
        "        print(f\"        Totale documenti: {len(updated_data)}\")\n",
        "        print(f\"        Nuovi aggiunti: {len(new_results)}\")\n",
        "\n",
        "    # 9) Summary finale\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"UPDATE INCREMENTALE COMPLETATO\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Documenti esistenti: {len(existing_data)}\")\n",
        "    print(f\"Nuovi trovati: {len(new_links)}\")\n",
        "    print(f\"Nuovi processati: {len(new_results)}\")\n",
        "    print(f\"Falliti: {len(failed)}\")\n",
        "\n",
        "    if failed:\n",
        "        print(f\"\\nDocumenti falliti:\")\n",
        "        for fail in failed:\n",
        "            print(f\"  - {fail['title'][:60]}: {fail['error']}\")\n",
        "\n",
        "    return {\n",
        "        'existing': len(existing_data),\n",
        "        'new_found': len(new_links),\n",
        "        'new_processed': len(new_results),\n",
        "        'failed': len(failed),\n",
        "        'failed_details': failed\n",
        "    }\n",
        "\n",
        "\n",
        "def interactive_incremental_scraping():\n",
        "    \"\"\"\n",
        "    Interfaccia interattiva per update incrementale\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OSHA INCREMENTAL UPDATE - Solo Nuovi Documenti\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Input query\n",
        "    query = input(\"Parola chiave o frase da cercare: \").strip()\n",
        "    if not query:\n",
        "        print(\"[ERROR] Query vuota!\")\n",
        "        return\n",
        "\n",
        "    lang = input(\"Lingua ('it' o 'en'): \").strip().lower()\n",
        "    while lang not in {\"it\", \"en\"}:\n",
        "        print(\"[ERROR] Lingua non valida!\")\n",
        "        lang = input(\"Lingua ('it' o 'en'): \").strip().lower()\n",
        "\n",
        "    # Nome file output\n",
        "    safe_query = re.sub(r'[^\\w\\-_.]', '_', query[:30])\n",
        "    default_file = f\"osha_{safe_query}_{lang}.json\"\n",
        "    output_file = input(f\"Nome file JSON (default: {default_file}): \").strip() or default_file\n",
        "\n",
        "    # Numero pagine\n",
        "    try:\n",
        "        max_pages = int(input(\"Max pagine di risultati (default 5): \").strip() or \"5\")\n",
        "    except ValueError:\n",
        "        max_pages = 5\n",
        "\n",
        "    # Check-only mode\n",
        "    check_only_input = input(\"Solo verifica senza aggiornare? (y/n, default: n): \").strip().lower()\n",
        "    check_only = (check_only_input == 'y')\n",
        "\n",
        "    # Costruisci URL\n",
        "    search_url = build_osha_search_url(query, lang=lang)\n",
        "    print(f\"\\n[INFO] URL: {search_url}\\n\")\n",
        "\n",
        "    # Esegui\n",
        "    stats = incremental_batch_scraping(\n",
        "        search_url=search_url,\n",
        "        output_file=output_file,\n",
        "        max_pages=max_pages,\n",
        "        check_only=check_only,\n",
        "        save_pdfs=True,\n",
        "        delay_between_docs=5\n",
        "    )\n",
        "\n",
        "    print(f\"\\n[DONE] Operazione completata!\")\n",
        "    return stats"
      ],
      "metadata": {
        "id": "HTHkAnjB0l9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check-Only Mode (Solo Verifica)"
      ],
      "metadata": {
        "id": "nh7lnPik0veq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica senza processare\n",
        "stats = incremental_batch_scraping(\n",
        "    search_url=build_osha_search_url(\"AI e lavoro\", lang=\"it\"),\n",
        "    output_file=\"osha_ai_lavoro.json\",\n",
        "    max_pages=3,\n",
        "    check_only=True  # ← Solo verifica\n",
        ")\n",
        "\n",
        "print(f\"Nuovi documenti disponibili: {stats['new_found']}\")\n"
      ],
      "metadata": {
        "id": "dZXHU5530s0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto-Update Mode (Processing Automatico)"
      ],
      "metadata": {
        "id": "N5Au9PhA09dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processa e aggiorna automaticamente\n",
        "stats = incremental_batch_scraping(\n",
        "    search_url=build_osha_search_url(\"workplace safety\", lang=\"en\"),\n",
        "    output_file=\"osha_workplace_safety_en.json\",\n",
        "    max_pages=5,\n",
        "    max_new_documents=10,  # Limite nuovi docs\n",
        "    check_only=False,\n",
        "    save_pdfs=False  # Non salvare PDF\n",
        ")"
      ],
      "metadata": {
        "id": "xwSmZdMc0_E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interactive Mode"
      ],
      "metadata": {
        "id": "ychmCXY01CQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Interfaccia interattiva\n",
        "    interactive_incremental_scraping()\n"
      ],
      "metadata": {
        "id": "dKD2EY9x1B2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}