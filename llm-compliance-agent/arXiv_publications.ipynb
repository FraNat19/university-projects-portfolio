{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **arXiv publications**"
      ],
      "metadata": {
        "id": "xL8seb2MBzZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "from urllib.parse import urljoin, quote_plus\n",
        "from bs4 import BeautifulSoup\n",
        "import fitz  # PyMuPDF"
      ],
      "metadata": {
        "id": "u2_CcreLBBn1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generic function to handle pdf**\n",
        "\n",
        "Takes a given pdf and extracts the content."
      ],
      "metadata": {
        "id": "b6rcYeMXB81M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scarica_e_leggi_pdf(pdf_url):\n",
        "    #Scarica un PDF da arXiv e restituisce il testo estratto\n",
        "    if not pdf_url:\n",
        "        return None\n",
        "\n",
        "    response = requests.get(pdf_url, stream=True)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Errore nel download del PDF: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    nome_file_pdf = 'arxiv_temp.pdf'\n",
        "    with open(nome_file_pdf, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    testo = []\n",
        "    with fitz.open(nome_file_pdf) as doc:\n",
        "        for page in doc:\n",
        "            testo.append(page.get_text())\n",
        "\n",
        "    full_text = re.sub(r'\\s+', ' ', \" \".join(testo)).strip()\n",
        "\n",
        "    # Elimina il file temporaneo\n",
        "    os.remove(nome_file_pdf)\n",
        "    return full_text\n"
      ],
      "metadata": {
        "id": "ASIoXdWXBFrj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search URL construction**\n",
        "\n",
        "Builds a normalized, case-insensitive search URL for the arXiv catalog using a free-text query (single or multi-word) and a target page index"
      ],
      "metadata": {
        "id": "0maFQP7ICrYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_arxiv_search_url(query: str, page: int = 0, size: int = 50):\n",
        "    # Crea l’URL per cercare articoli su arXiv\n",
        "\n",
        "    encoded_query = quote_plus(query.strip())\n",
        "    start = page * size\n",
        "    return f\"https://arxiv.org/search/?query={encoded_query}&searchtype=all&abstracts=show&order=-announced_date_first&size={size}&start={start}\"\n"
      ],
      "metadata": {
        "id": "X_Be0e8cBIYK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link and base metadata extraction**\n",
        "\n",
        "Parses a results page to collect all publication cards and extracts each card’s title and absolute URL."
      ],
      "metadata": {
        "id": "t_T5lhubC9FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_arxiv_links(query: str, max_pages: int = 1):\n",
        "    # Estrae tutti i link e i titoli dalle pagine di ricerca arXiv\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    all_links = []\n",
        "\n",
        "    for p in range(max_pages):\n",
        "        url = build_arxiv_search_url(query, page=p)\n",
        "        print(f\" Pagina {p+1}/{max_pages}: {url}\")\n",
        "\n",
        "        res = requests.get(url, headers=headers)\n",
        "        if res.status_code != 200:\n",
        "            print(f\"Errore nella pagina {p+1}: {res.status_code}\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(res.content, 'lxml')\n",
        "        results = soup.find_all('li', class_='arxiv-result')\n",
        "\n",
        "        if not results:\n",
        "            print(\"Nessun risultato trovato.\")\n",
        "            break\n",
        "\n",
        "        for r in results:\n",
        "            title_elem = r.find('p', class_='title')\n",
        "            link_elem = r.find('p', class_='list-title')\n",
        "            abs_elem = r.find('span', class_='abstract-full')\n",
        "\n",
        "            if link_elem and link_elem.find('a'):\n",
        "                link = link_elem.find('a')['href']\n",
        "                all_links.append({\n",
        "                    \"titolo\": title_elem.text.strip() if title_elem else \"N/A\",\n",
        "                    \"url\": link.strip(),\n",
        "                    \"abstract\": abs_elem.text.strip() if abs_elem else \"N/A\"\n",
        "                })\n",
        "\n",
        "        time.sleep(random.uniform(1, 2))  # piccola pausa per sicurezza\n",
        "\n",
        "    return all_links\n"
      ],
      "metadata": {
        "id": "CC_RtjuSBMdq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Article-level details extraction**\n",
        "\n",
        "Given an article page, extracts core metadata (title, publication date, PDF link) and, when available, the PDF’s full text."
      ],
      "metadata": {
        "id": "v26Mi-g0DH75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_arxiv_details(article_url):\n",
        "    # Estrae metadati e testo PDF da un articolo arXiv\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    p = requests.get(article_url, headers=headers)\n",
        "    page = BeautifulSoup(p.content, 'lxml')\n",
        "\n",
        "    # Titolo\n",
        "    title_elem = page.find('h1', class_='title')\n",
        "    titolo = title_elem.text.replace('Title:', '').strip() if title_elem else \"N/A\"\n",
        "\n",
        "    # Autori\n",
        "    authors_elem = page.find('div', class_='authors')\n",
        "    autori = authors_elem.text.replace('Authors:', '').strip() if authors_elem else \"N/A\"\n",
        "\n",
        "    # Data\n",
        "    date_elem = page.find('div', class_='dateline')\n",
        "    data = date_elem.text.strip().replace('Submitted ', '') if date_elem else \"N/A\"\n",
        "\n",
        "    # Abstract\n",
        "    abs_elem = page.find('blockquote', class_='abstract')\n",
        "    abstract = abs_elem.text.replace('Abstract:', '').strip() if abs_elem else \"N/A\"\n",
        "\n",
        "    # PDF link\n",
        "    pdf_a = page.find('a', string=lambda s: s and s.strip().lower() == 'pdf')\n",
        "    pdf_url = urljoin(\"https://arxiv.org\", pdf_a['href']) if pdf_a else None\n",
        "\n",
        "    # Estrazione testo PDF\n",
        "    testo_pdf = scarica_e_leggi_pdf(pdf_url) if pdf_url else None\n",
        "\n",
        "    return {\n",
        "        \"titolo\": titolo,\n",
        "        \"autori\": autori,\n",
        "        \"data\": data,\n",
        "        \"abstract\": abstract,\n",
        "        \"pdf_url\": pdf_url,\n",
        "        \"testo_pdf\": testo_pdf,\n",
        "        \"url_pubblicazione\": article_url\n",
        "    }"
      ],
      "metadata": {
        "id": "Ww3SuM1ABQAD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**JSON saving**"
      ],
      "metadata": {
        "id": "-L6HfqzoDO_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def salva_in_json(record, filename):\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "    else:\n",
        "        data = []\n",
        "    data.append(record)\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "N-xUyfq-BS_3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interactive function**\n",
        "\n",
        "Prompts the user for a keyword (and optionally a publication date range), builds the corresponding search URLs for the requested number of pages, scrapes results, and saves them to JSON."
      ],
      "metadata": {
        "id": "bRA45kuUDWRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_arxiv_interattivo(output_file=\"arxiv_risultati.json\"):\n",
        "    # Chiede all’utente la query e il numero di pagine, poi estrae tutto\n",
        "    print(\"RICERCA PUBBLICAZIONI ARXIV\")\n",
        "    query = input(\" Inserisci la parola o frase da cercare: \").strip()\n",
        "    while not query:\n",
        "        query = input(\"La query non può essere vuota. Riprova: \").strip()\n",
        "\n",
        "    try:\n",
        "        max_pages = int(input(\" Inserisci il numero di pagine da analizzare: \").strip())\n",
        "        if max_pages < 1:\n",
        "            raise ValueError\n",
        "    except ValueError:\n",
        "        max_pages = 1\n",
        "        print(\"Numero non valido. Analizzerò solo 1 pagina.\")\n",
        "\n",
        "    print(\" Cerco le pubblicazioni, attendi...\\n\")\n",
        "    links = get_arxiv_links(query, max_pages=max_pages)\n",
        "    print(f\"\\nTrovate {len(links)} pubblicazioni totali.\\n\")\n",
        "\n",
        "    for i, item in enumerate(links, start=1):\n",
        "        print(f\"[{i}/{len(links)}] Estraggo: {item['titolo']}\")\n",
        "        try:\n",
        "            dati = scrape_arxiv_details(item['url'])\n",
        "            salva_in_json(dati, output_file)\n",
        "            print(f\" Salvato: {item['titolo']}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\" Errore su {item['url']}: {e}\")\n",
        "        time.sleep(random.uniform(2, 4))\n",
        "\n",
        "    print(f\" Tutti i dati salvati in '{output_file}'.\")"
      ],
      "metadata": {
        "id": "IxWxJ3GEBWHt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test the function**"
      ],
      "metadata": {
        "id": "Sltt1duxDi8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_arxiv_interattivo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "fpCIL0vv_HRH",
        "outputId": "d30e2ed8-66b0-44e5-a836-59148788a991"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RICERCA PUBBLICAZIONI ARXIV\n",
            " Inserisci la parola o frase da cercare: mental health\n",
            " Inserisci il numero di pagine da analizzare: 2\n",
            " Cerco le pubblicazioni, attendi...\n",
            "\n",
            " Pagina 1/2: https://arxiv.org/search/?query=mental+health&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=0\n",
            " Pagina 2/2: https://arxiv.org/search/?query=mental+health&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=50\n",
            "\n",
            "Trovate 100 pubblicazioni totali.\n",
            "\n",
            "[1/100] Estraggo: A Case for Leveraging Generative AI to Expand and Enhance Training in the Provision of Mental Health Services\n",
            " Salvato: A Case for Leveraging Generative AI to Expand and Enhance Training in the Provision of Mental Health Services\n",
            "\n",
            "[2/100] Estraggo: Position: AI Will Transform Neuropsychology Through Mental Health Digital Twins for Dynamic Mental Health Care, Especially for ADHD\n",
            " Salvato: Position: AI Will Transform Neuropsychology Through Mental Health Digital Twins for Dynamic Mental Health Care, Especially for ADHD\n",
            "\n",
            "[3/100] Estraggo: Lonely Individuals Show Distinct Patterns of Social Media Engagement\n",
            " Salvato: Lonely Individuals Show Distinct Patterns of Social Media Engagement\n",
            "\n",
            "[4/100] Estraggo: Beyond Motion Artifacts: Optimizing PPG Preprocessing for Accurate Pulse Rate Variability Estimation\n",
            " Salvato: Beyond Motion Artifacts: Optimizing PPG Preprocessing for Accurate Pulse Rate Variability Estimation\n",
            "\n",
            "[5/100] Estraggo: Two Modes of Reflection: How Temporal, Spatial, and Social Distances Affect Reflective Writing in Family Caregiving\n",
            " Salvato: Two Modes of Reflection: How Temporal, Spatial, and Social Distances Affect Reflective Writing in Family Caregiving\n",
            "\n",
            "[6/100] Estraggo: Cross-Lingual Mental Health Ontologies for Indian Languages: Bridging Patient Expression and Clinical Understanding through Explainable AI and Human-in-the-Loop Validation\n",
            " Salvato: Cross-Lingual Mental Health Ontologies for Indian Languages: Bridging Patient Expression and Clinical Understanding through Explainable AI and Human-in-the-Loop Validation\n",
            "\n",
            "[7/100] Estraggo: Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks\n",
            " Salvato: Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks\n",
            "\n",
            "[8/100] Estraggo: Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model\n",
            " Salvato: Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model\n",
            "\n",
            "[9/100] Estraggo: DEAP DIVE: Dataset Investigation with Vision transformers for EEG evaluation\n",
            " Salvato: DEAP DIVE: Dataset Investigation with Vision transformers for EEG evaluation\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-402765743.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscrape_arxiv_interattivo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-196828249.py\u001b[0m in \u001b[0;36mscrape_arxiv_interattivo\u001b[0;34m(output_file)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Errore su {item['url']}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Tutti i dati salvati in '{output_file}'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fermata volontariamente ai primi 10 giusto per provare che funzionasse, informazioni scaricate molto bene"
      ],
      "metadata": {
        "id": "b0h_rLZlD5Cb"
      }
    }
  ]
}