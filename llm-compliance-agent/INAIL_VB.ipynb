{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INAIL SCRAPER**\n",
        "\n",
        "AUTORE: Francesco Natali\n",
        "DATA: Gennaio 2025\n",
        "SCOPO: Scraping automatico e intelligente delle pubblicazioni INAIL per tesi\n",
        "\n",
        "CARATTERISTICHE:\n",
        "- âœ… Scraping incrementale (skippa documenti giÃ  processati)\n",
        "- âœ… Estrazione automatica di PDF con Docling\n",
        "- âœ… Analisi VLM delle immagini (SmolVLM)\n",
        "- âœ… Estrazione strutturata di tabelle e figure\n",
        "- âœ… Backup automatico su Google Drive\n",
        "- âœ… Manifest JSON per Vector Database\n",
        "\n",
        "REQUISITI SISTEMA:\n",
        "- Google Colab (con GPU per VLM)\n",
        "- Google Drive montato\n",
        "- Connessione internet stabile\n",
        "\n",
        "UTILIZZO:\n",
        "1. Eseguire setup iniziale\n",
        "2. Scegliere modalitÃ  di scraping\n",
        "3. Lo script gestisce automaticamente backup e recovery"
      ],
      "metadata": {
        "id": "rDU8NjW5hzp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SETUP E INSTALLAZIONE DIPENDENZE\n",
        "\n",
        "# Installa pacchetti necessari\n",
        "# %pip install -q google-colab-selenium docling \"docling[vlm]\" transformers torch pillow accelerate\n",
        "\n",
        "# Import librerie standard\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, Optional, List\n",
        "from urllib.parse import urljoin, quote_plus\n",
        "\n",
        "# Import librerie per scraping\n",
        "import google_colab_selenium as gs\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "9go2LC5uiH5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURAZIONE GLOBALE\n",
        "\n",
        "class INAILConfig:\n",
        "\n",
        "    # Configurazione centralizzata per lo scraper INAIL. Gestisce Directory di output (locale e Drive), Impostazioni VLM (Vision Language Model), Prompt per analisi immagini\n",
        "\n",
        "\n",
        "    # Directory locali (su istanza Colab)\n",
        "    OUTPUT_DIR = Path('./inail_scraped_data')\n",
        "    PDF_DIR = OUTPUT_DIR / 'pdfs'\n",
        "    JSON_DIR = OUTPUT_DIR / 'json'\n",
        "    IMAGES_DIR = OUTPUT_DIR / 'images'\n",
        "    TABLES_DIR = OUTPUT_DIR / 'tables'\n",
        "\n",
        "    # Directory backup su Google Drive\n",
        "    DRIVE_BACKUP_DIR = Path('/content/drive/MyDrive/INAIL_Thesis_Data')\n",
        "\n",
        "    # Configurazione Vision Language Model\n",
        "    VLM_ENABLED = True\n",
        "    VLM_MODEL = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n",
        "    VLM_PROMPT = \"\"\"Describe this technical document image in detail for academic research. Focus on:\n",
        "- Main content type (diagram, chart, photo, schematic)\n",
        "- Key visual elements and their relationships\n",
        "- Any visible text, labels, or numerical data\n",
        "- Technical details relevant to workplace safety or industrial processes\n",
        "Be precise and comprehensive.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def setup_directories(cls):\n",
        "        \"\"\"Crea tutte le directory necessarie se non esistono\"\"\"\n",
        "        for dir_path in [cls.OUTPUT_DIR, cls.PDF_DIR, cls.JSON_DIR,\n",
        "                        cls.IMAGES_DIR, cls.TABLES_DIR]:\n",
        "            dir_path.mkdir(exist_ok=True, parents=True)\n",
        "        print(f\"[INFO] âœ… Directory di output pronte: {cls.OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "pMp9pWg5iPI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INIZIALIZZAZIONE SELENIUM DRIVER\n",
        "\n",
        "def initialize_selenium_driver():\n",
        "\n",
        "    # Inizializza il browser Selenium con configurazione ottimizzata\n",
        "    # Configurazioni Headless mode (nessuna finestra visibile), Timeout aumentati per stabilitÃ , User agent realistico\n",
        "    # Returns driver: Istanza di Chrome WebDriver\n",
        "\n",
        "    print(\"[SETUP] Inizializzazione Selenium WebDriver...\")\n",
        "\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument('--disable-software-rasterizer')\n",
        "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
        "\n",
        "    driver = gs.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(180)  # 3 minuti per page load\n",
        "    driver.implicitly_wait(20)  # Wait implicito 20 secondi\n",
        "\n",
        "    print(\"[SETUP] âœ… Driver Selenium pronto\\n\")\n",
        "    return driver\n",
        "\n",
        "# Inizializza driver globale\n",
        "driver = initialize_selenium_driver()\n",
        "BASE_CATALOG_URL = \"https://www.inail.it/portale/it/inail-comunica/pubblicazioni/catalogo-generale.html\"\n"
      ],
      "metadata": {
        "id": "FiAeF-Fziety"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOGLE DRIVE - BACKUP E RIPRISTINO\n",
        "\n",
        "def mount_google_drive():\n",
        "\n",
        "    # Monta Google Drive nell'ambiente Colab.\n",
        "    # Necessario per backup persistente dei dati.\n",
        "\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        if not Path('/content/drive').exists():\n",
        "            drive.mount('/content/drive', force_remount=False)\n",
        "            print(\"[SUCCESS] âœ… Google Drive montato\")\n",
        "        else:\n",
        "            print(\"[INFO] Google Drive giÃ  montato\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] âš ï¸ Impossibile montare Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def backup_to_drive():\n",
        "\n",
        "    # Esegue backup completo di tutti i dati scrapati su Google Drive\n",
        "    # Backup include PDF originali, JSON con metadati, Immagini estratte + descrizioni VLM, Tabelle estratte, Manifest per vector database\n",
        "    # Returns bool: True se backup riuscito, False altrimenti\n",
        "\n",
        "    source = INAILConfig.OUTPUT_DIR\n",
        "    dest = INAILConfig.DRIVE_BACKUP_DIR\n",
        "\n",
        "    if not source.exists():\n",
        "        print(\"[ERROR] âŒ Nessun dato da backuppare\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        if not Path('/content/drive').exists():\n",
        "            mount_google_drive()\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"BACKUP SU GOOGLE DRIVE\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Origine: {source}\")\n",
        "        print(f\"Destinazione: {dest}\\n\")\n",
        "\n",
        "        # Copia ricorsiva di tutti i file\n",
        "        shutil.copytree(str(source), str(dest), dirs_exist_ok=True)\n",
        "\n",
        "        # Statistiche backup\n",
        "        total_files = sum(1 for _ in dest.rglob('*') if _.is_file())\n",
        "        total_size_mb = sum(f.stat().st_size for f in dest.rglob('*') if f.is_file()) / (1024**2)\n",
        "\n",
        "        print(f\"âœ… Backup completato!\")\n",
        "        print(f\"ðŸ“¦ File totali: {total_files}\")\n",
        "        print(f\"ðŸ’¾ Dimensione: {total_size_mb:.1f} MB\")\n",
        "        print(f\"ðŸ“ Path Drive: {dest}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] âŒ Backup fallito: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def restore_from_drive():\n",
        "    # Ripristina dati da backup Drive\n",
        "    # Returns bool: True se ripristino riuscito, False altrimenti\n",
        "\n",
        "    source = INAILConfig.DRIVE_BACKUP_DIR\n",
        "    dest = INAILConfig.OUTPUT_DIR\n",
        "\n",
        "    try:\n",
        "        if not Path('/content/drive').exists():\n",
        "            mount_google_drive()\n",
        "\n",
        "        if not source.exists():\n",
        "            print(\"[INFO] â„¹ï¸ Nessun backup trovato su Drive (prima esecuzione)\")\n",
        "            return False\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"RIPRISTINO DA GOOGLE DRIVE\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        shutil.copytree(str(source), str(dest), dirs_exist_ok=True)\n",
        "\n",
        "        total_files = sum(1 for _ in dest.rglob('*') if _.is_file())\n",
        "\n",
        "        print(f\"âœ… Ripristino completato!\")\n",
        "        print(f\"ðŸ“¦ File ripristinati: {total_files}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] âŒ Ripristino fallito: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "PWVjvdEriu2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOCLING - PROCESSAMENTO PDF\n",
        "\n",
        "def initialize_docling_converter():\n",
        "\n",
        "    # Inizializza Docling per estrazione contenuti da PDF\n",
        "    # Docling Ã¨ lo stato dell'arte per Estrazione testo preservando struttura, Estrazione immagini ad alta risoluzione, OCR su documenti scansionati, Riconoscimento tabelle\n",
        "    # Returns converter: Istanza di DocumentConverter\n",
        "\n",
        "    from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "    from docling.datamodel.base_models import InputFormat\n",
        "    from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "\n",
        "    # Configurazione pipeline\n",
        "    pipeline_options = PdfPipelineOptions()\n",
        "    pipeline_options.generate_picture_images = True  # Estrai immagini\n",
        "    pipeline_options.images_scale = 2.0  # Scala 2x per qualitÃ \n",
        "    pipeline_options.do_ocr = True  # OCR su immagini\n",
        "    pipeline_options.do_picture_description = False  # Gestito da VLM\n",
        "\n",
        "    converter = DocumentConverter(\n",
        "        format_options={\n",
        "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
        "        }\n",
        "    )\n",
        "\n",
        "    vlm_status = \"ON\" if INAILConfig.VLM_ENABLED else \"OFF\"\n",
        "    print(f\"[INFO] âœ… Docling pronto | VLM: {vlm_status}\")\n",
        "\n",
        "    return converter"
      ],
      "metadata": {
        "id": "0TSTTqxBjK-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HELPER FUNCTIONS - UTILITÃ€ GENERALI\n",
        "\n",
        "def download_pdf_inail(pdf_url: str, output_path: Path) -> bool:\n",
        "\n",
        "    # Scarica PDF da URL INAIL\n",
        "    # Args: pdf_url URL del PDF da scaricare, output_path Path dove salvare il file\n",
        "    # Returns: True se download riuscito\n",
        "\n",
        "    import os\n",
        "    try:\n",
        "        os.system(f'wget -q -O {output_path} \"{pdf_url}\"')\n",
        "        return output_path.exists() and output_path.stat().st_size > 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "def normalize_query(q: str) -> str:\n",
        "    # Normalizza query di ricerca per URL encoding\n",
        "    q = unicodedata.normalize(\"NFKC\", q).strip().lower()\n",
        "    return \" \".join(q.split()).replace(\"'\", \"'\")\n",
        "\n",
        "\n",
        "def validate_date(date_str: str) -> bool:\n",
        "    #Valida formato data dd/mm/yyyy\n",
        "    return bool(re.match(r\"^\\d{2}/\\d{2}/\\d{4}$\", date_str))\n",
        "\n",
        "\n",
        "def build_inail_search_url(query: str = None, page: int = 1,\n",
        "                          start_date: str = None, end_date: str = None) -> str:\n",
        "\n",
        "    # Costruisce URL di ricerca catalogo INAIL\n",
        "    # Args: query Termini di ricerca (opzionale), page Numero pagina (default: 1), start_date Data inizio in formato dd/mm/yyyy, end_date Data fine in formato dd/mm/yyyy\n",
        "    # Returns: str URL completo per la ricerca\n",
        "\n",
        "    if page < 1:\n",
        "        raise ValueError(\"Pagina deve essere >= 1\")\n",
        "\n",
        "    params = []\n",
        "    if query and query.strip():\n",
        "        params.append(f\"text={quote_plus(normalize_query(query))}\")\n",
        "    if start_date and validate_date(start_date):\n",
        "        params.append(f\"startDate={quote_plus(start_date)}\")\n",
        "    if end_date and validate_date(end_date):\n",
        "        params.append(f\"endDate={quote_plus(end_date)}\")\n",
        "    params.append(f\"page={page}\")\n",
        "\n",
        "    return f\"{BASE_CATALOG_URL}?{'&'.join(params)}\""
      ],
      "metadata": {
        "id": "JxPTTn57jYv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DRIVER MANAGEMENT - GESTIONE CRASH SELENIUM\n",
        "\n",
        "def recreate_driver():\n",
        "\n",
        "    # Ricrea il driver Selenium in caso di timeout/disconnessione\n",
        "    # Gestisce automaticamente Chiusura driver corrotto, Creazione nuovo driver, Attesa stabilizzazione\n",
        "    # Returns: driver Nuovo driver Selenium\n",
        "\n",
        "    global driver\n",
        "\n",
        "    try:\n",
        "        driver.quit()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print(\"\\n[INFO] â™»ï¸  Ricreo driver Selenium...\")\n",
        "\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    options.add_argument('--disable-gpu')\n",
        "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
        "\n",
        "    driver = gs.Chrome(options=options)\n",
        "    driver.set_page_load_timeout(180)\n",
        "    driver.implicitly_wait(20)\n",
        "\n",
        "    time.sleep(5)  # Stabilizzazione\n",
        "    print(\"[INFO] âœ… Driver ricreato\\n\")\n",
        "\n",
        "    return driver"
      ],
      "metadata": {
        "id": "TME1wzoLj6zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LINK EXTRACTION - ESTRAZIONE PUBBLICAZIONI\n",
        "\n",
        "def extract_cards_with_wait(driver, timeout: int = 20, max_retries: int = 4) -> List[Dict[str, str]]:\n",
        "\n",
        "    # Estrae card delle pubblicazioni da una pagina di risultati\n",
        "    # Gestisce: Retry automatici su timeout, Refresh pagina in caso di errori, Parsing robusto dell'HTML\n",
        "    # Args: driver Istanza Selenium WebDriver, timeout Timeout in secondi (default: 20), max_retries Numero massimo di tentativi (default: 4)\n",
        "    # Returns: Lista di pubblicazioni trovate [{titolo, url}, ...]\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"    [Tentativo {attempt + 1}/{max_retries}]\", end=\" \")\n",
        "\n",
        "            # Attendi caricamento elementi\n",
        "            WebDriverWait(driver, timeout).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"h3.card-title a[href], body\"))\n",
        "            )\n",
        "\n",
        "            time.sleep(3)  # Pausa per rendering JavaScript\n",
        "\n",
        "            # Estrai card\n",
        "            results = []\n",
        "            cards_elements = driver.find_elements(By.CSS_SELECTOR, \"h3.card-title a[href]\")\n",
        "\n",
        "            for a in cards_elements:\n",
        "                try:\n",
        "                    href = a.get_attribute(\"href\")\n",
        "                    text = a.text.strip()\n",
        "                    if href and text:\n",
        "                        results.append({\n",
        "                            \"titolo\": text,\n",
        "                            \"url\": urljoin(\"https://www.inail.it\", href)\n",
        "                        })\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if results:\n",
        "                print(f\"âœ“ {len(results)} card\")\n",
        "                return results\n",
        "            else:\n",
        "                print(\"âœ— Nessuna card\")\n",
        "\n",
        "        except TimeoutException:\n",
        "            print(f\"âœ— Timeout\")\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = random.uniform(10, 15)\n",
        "                print(f\"    [Pausa {wait_time:.1f}s]\")\n",
        "                time.sleep(wait_time)\n",
        "                try:\n",
        "                    driver.refresh()\n",
        "                    time.sleep(5)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Errore: {str(e)[:50]}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(8)\n",
        "\n",
        "    print(\"    [FALLITO]\")\n",
        "    return []\n",
        "\n",
        "\n",
        "def get_inail_publication_links(driver, query: str = None, max_pages: int = 1,\n",
        "                               start_date: str = None, end_date: str = None) -> List[Dict[str, str]]:\n",
        "\n",
        "    # Estrae link di tutte le pubblicazioni INAIL da piÃ¹ pagine di risultati.\n",
        "    # Features: Stop intelligente su pagine vuote consecutive, Auto-retry su timeout/connection errors, Ricreazione automatica driver se necessario, Deduplicazione link\n",
        "    # Args: driver Istanza Selenium WebDriver, query Termini di ricerca (opzionale), max_pages Numero massimo di pagine da esplorare, end_date: Data fine filtraggio (dd/mm/yyyy)\n",
        "    # Returns: Lista completa pubblicazioni trovate\n",
        "\n",
        "    all_links = []\n",
        "    seen = set()\n",
        "    consecutive_failures = 0\n",
        "    MAX_FAILURES = 2\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ESTRAZIONE LINK PUBBLICAZIONI\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Query: {query if query else '(tutte)'} | Max pagine: {max_pages}\\n\")\n",
        "\n",
        "    for page_num in range(1, max_pages + 1):\n",
        "        url = build_inail_search_url(query=query, page=page_num,\n",
        "                                     start_date=start_date, end_date=end_date)\n",
        "\n",
        "        print(f\"[Pagina {page_num}/{max_pages}]\")\n",
        "\n",
        "        try:\n",
        "            driver.get(url)\n",
        "\n",
        "            # Pausa tra pagine per evitare rate limiting\n",
        "            if page_num > 1:\n",
        "                delay = random.uniform(6, 10)\n",
        "                print(f\"  [Pausa {delay:.1f}s]\")\n",
        "                time.sleep(delay)\n",
        "\n",
        "            # Estrai card da pagina\n",
        "            cards = extract_cards_with_wait(driver, timeout=20)\n",
        "\n",
        "            if not cards:\n",
        "                consecutive_failures += 1\n",
        "                print(f\"  âš ï¸ Pagina vuota ({consecutive_failures}/{MAX_FAILURES})\")\n",
        "\n",
        "                if consecutive_failures >= MAX_FAILURES:\n",
        "                    print(f\"\\n  ðŸ›‘ STOP: Troppe pagine vuote consecutive\")\n",
        "                    break\n",
        "\n",
        "                time.sleep(random.uniform(10, 15))\n",
        "                continue\n",
        "\n",
        "            # Reset contatore failures se trovate card\n",
        "            consecutive_failures = 0\n",
        "            new_count = 0\n",
        "\n",
        "            # Aggiungi card non duplicate\n",
        "            for card in cards:\n",
        "                if card[\"url\"] not in seen:\n",
        "                    seen.add(card[\"url\"])\n",
        "                    all_links.append(card)\n",
        "                    new_count += 1\n",
        "\n",
        "            print(f\"  âœ… +{new_count} nuove (totale: {len(all_links)})\")\n",
        "            time.sleep(random.uniform(2, 4))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(f\"\\nâš ï¸ INTERRUZIONE MANUALE\")\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            consecutive_failures += 1\n",
        "            error_msg = str(e)\n",
        "            print(f\"  âŒ Errore: {error_msg[:100]}\")\n",
        "\n",
        "            # Gestione errori di connessione/timeout\n",
        "            if any(keyword in error_msg.lower() for keyword in\n",
        "                   ['timeout', 'connection', 'disconnected', 'chrome']):\n",
        "                print(f\"  âš ï¸  Problema con driver, ricreo...\")\n",
        "                try:\n",
        "                    driver = recreate_driver()\n",
        "                    consecutive_failures = 0\n",
        "                    print(f\"  â™»ï¸  Ritento pagina {page_num}...\")\n",
        "                    continue\n",
        "                except Exception as recreate_error:\n",
        "                    print(f\"  âŒ Impossibile ricreare driver: {recreate_error}\")\n",
        "                    break\n",
        "\n",
        "            if consecutive_failures >= MAX_FAILURES:\n",
        "                print(f\"\\n  ðŸ›‘ STOP: Troppi errori consecutivi\")\n",
        "                break\n",
        "\n",
        "            time.sleep(random.uniform(12, 18))\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ðŸ“Š Totale pubblicazioni trovate: {len(all_links)}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return all_links"
      ],
      "metadata": {
        "id": "bl-dh_sLk07i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOCUMENT PROCESSING - ANALISI STRUTTURA DOCUMENTI\n",
        "\n",
        "def analyze_document_structure(doc) -> Dict[str, Any]:\n",
        "\n",
        "    # Analizza struttura del documento PDF estratto da Docling\n",
        "    # Estrae: Headings (titoli di sezione) con livello gerarchico, Tabelle con metadati (caption, colonne, righe), Figure con caption e posizione\n",
        "    # Args: Documento Docling processato\n",
        "    # Returns: Dict Struttura completa del documento\n",
        "\n",
        "    structure = {\n",
        "        'num_tables': 0,\n",
        "        'num_figures': 0,\n",
        "        'num_headings': 0,\n",
        "        'headings': [],\n",
        "        'tables': [],\n",
        "        'figures': []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        markdown = doc.export_to_markdown()\n",
        "        lines = markdown.split('\\n')\n",
        "\n",
        "        # ESTRAZIONE HEADINGS\n",
        "        for i, line in enumerate(lines):\n",
        "            if line.strip().startswith('#'):\n",
        "                level = len(line) - len(line.lstrip('#'))\n",
        "                text = line.lstrip('#').strip()\n",
        "                structure['num_headings'] += 1\n",
        "                structure['headings'].append({\n",
        "                    'type': f'heading_level_{level}',\n",
        "                    'text': text[:200],  # Limita lunghezza\n",
        "                    'position': i,\n",
        "                    'level': level\n",
        "                })\n",
        "\n",
        "        # ESTRAZIONE TABELLE\n",
        "        in_table = False\n",
        "        current_table_lines = []\n",
        "        table_start_idx = -1\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            # Rileva inizio tabella (linea con pipe e trattini)\n",
        "            if '|' in line and ('-' in line or 'â”€' in line):\n",
        "                if not in_table:\n",
        "                    in_table = True\n",
        "                    table_start_idx = i\n",
        "                    current_table_lines = []\n",
        "\n",
        "            # Accumula righe tabella\n",
        "            if in_table:\n",
        "                if '|' in line:\n",
        "                    current_table_lines.append(line)\n",
        "                else:\n",
        "                    # Fine tabella\n",
        "                    if current_table_lines:\n",
        "                        # Cerca caption nelle righe precedenti\n",
        "                        caption = 'N/A'\n",
        "                        for j in range(max(0, table_start_idx-5), table_start_idx):\n",
        "                            candidate = lines[j].strip()\n",
        "                            if candidate and 'Tabella' in candidate:\n",
        "                                caption = candidate\n",
        "                                break\n",
        "\n",
        "                        # Estrai colonne dalla prima riga\n",
        "                        potential_columns = []\n",
        "                        if current_table_lines:\n",
        "                            first_row = current_table_lines[0]\n",
        "                            cols = [c.strip() for c in first_row.split('|') if c.strip()]\n",
        "                            potential_columns = [col for col in cols\n",
        "                                               if col and col not in ['-', 'â”€']]\n",
        "\n",
        "                        structure['num_tables'] += 1\n",
        "                        structure['tables'].append({\n",
        "                            'table_id': f'table_{structure[\"num_tables\"]}',\n",
        "                            'caption': caption,\n",
        "                            'text_content': '\\n'.join(current_table_lines),\n",
        "                            'num_rows': len(current_table_lines),\n",
        "                            'num_columns': len(potential_columns),\n",
        "                            'potential_columns': potential_columns,\n",
        "                            'position': table_start_idx\n",
        "                        })\n",
        "\n",
        "                    in_table = False\n",
        "                    current_table_lines = []\n",
        "\n",
        "        # ESTRAZIONE FIGURE\n",
        "        for i, line in enumerate(lines):\n",
        "            if '<!-- image -->' in line.lower() or \\\n",
        "               (line.strip().startswith('![') and '](' in line):\n",
        "                structure['num_figures'] += 1\n",
        "\n",
        "                # Cerca caption nelle righe successive\n",
        "                caption = 'N/A'\n",
        "                for j in range(i+1, min(len(lines), i+5)):\n",
        "                    candidate = lines[j].strip()\n",
        "                    if candidate and not candidate.startswith('#'):\n",
        "                        caption = candidate\n",
        "                        break\n",
        "\n",
        "                structure['figures'].append({\n",
        "                    'figure_id': f'figure_{structure[\"num_figures\"]}',\n",
        "                    'caption': caption,\n",
        "                    'position': i,\n",
        "                    'vlm_description': None  # VerrÃ  popolato da VLM\n",
        "                })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[!] Errore analisi struttura: {e}\")\n",
        "\n",
        "    return structure\n",
        "\n",
        "\n",
        "def export_images_from_document(doc, doc_id: str) -> List[Dict[str, Any]]:\n",
        "\n",
        "    # Estrae e salva tutte le immagini da un documento Docling.\n",
        "    # Args: doc Documento Docling processato, doc_id Identificativo univoco documento\n",
        "    # Returns: Lista metadati immagini esportate\n",
        "\n",
        "    images_dir = INAILConfig.IMAGES_DIR / doc_id\n",
        "    images_dir.mkdir(parents=True, exist_ok=True)\n",
        "    exported_images = []\n",
        "\n",
        "    try:\n",
        "        from docling_core.types.doc import PictureItem\n",
        "\n",
        "        if hasattr(doc, 'iterate_items'):\n",
        "            for element, _level in doc.iterate_items():\n",
        "                if isinstance(element, PictureItem):\n",
        "                    picture_counter = len(exported_images) + 1\n",
        "                    img_filename = f\"figure_{picture_counter}.png\"\n",
        "                    img_path = images_dir / img_filename\n",
        "\n",
        "                    try:\n",
        "                        pil_image = element.get_image(doc)\n",
        "                        pil_image.save(str(img_path), \"PNG\")\n",
        "\n",
        "                        exported_images.append({\n",
        "                            'path': str(img_path),\n",
        "                            'filename': img_filename,\n",
        "                            'caption': getattr(element, 'caption', 'N/A'),\n",
        "                            'position': len(exported_images),\n",
        "                            'vlm_description': None  # Popolato dopo\n",
        "                        })\n",
        "\n",
        "                        print(f\"  [+] Immagine salvata: {img_filename}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"  [!] Errore salvataggio figura: {e}\")\n",
        "\n",
        "        print(f\"[OK] {len(exported_images)} immagini esportate\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Export immagini fallito: {e}\")\n",
        "\n",
        "    return exported_images\n",
        "\n",
        "\n",
        "def export_tables_to_files(tables: List[Dict], doc_id: str) -> List[str]:\n",
        "\n",
        "    # Salva tabelle estratte come file di testo con metadati\n",
        "    # Ogni tabella viene salvata come file .txt contenente Caption e metadati, Colonne identificate, Contenuto completo della tabella\n",
        "    # Args: tables Lista tabelle estratte, doc_id Identificativo univoco documento\n",
        "    # Returns: Path dei file tabelle creati\n",
        "\n",
        "    if not tables:\n",
        "        return []\n",
        "\n",
        "    tables_dir = INAILConfig.TABLES_DIR / doc_id\n",
        "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
        "    exported_tables = []\n",
        "\n",
        "    for table in tables:\n",
        "        table_id = table['table_id']\n",
        "        txt_path = tables_dir / f\"{table_id}.txt\"\n",
        "\n",
        "        try:\n",
        "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"=\"*80 + \"\\n\")\n",
        "                f.write(f\"TABELLA: {table_id}\\n\")\n",
        "                f.write(\"=\"*80 + \"\\n\\n\")\n",
        "                f.write(f\"CAPTION: {table.get('caption', 'N/A')}\\n\\n\")\n",
        "\n",
        "                if table.get('potential_columns'):\n",
        "                    f.write(\"COLONNE IDENTIFICATE:\\n\")\n",
        "                    for idx, col in enumerate(table['potential_columns'], 1):\n",
        "                        f.write(f\"  {idx}. {col}\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "\n",
        "                f.write(\"METADATI STRUTTURALI:\\n\")\n",
        "                f.write(f\"  - Righe: {table.get('num_rows', 'N/A')}\\n\")\n",
        "                f.write(f\"  - Colonne: {len(table.get('potential_columns', []))}\\n\")\n",
        "                f.write(f\"  - Posizione: {table.get('position', 'N/A')}\\n\\n\")\n",
        "\n",
        "                f.write(\"CONTENUTO:\\n\" + \"-\"*80 + \"\\n\")\n",
        "                f.write(table['text_content'])\n",
        "                f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "            exported_tables.append(str(txt_path))\n",
        "            print(f\"  [+] Tabella salvata: {txt_path.name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [!] Errore salvataggio tabella: {e}\")\n",
        "\n",
        "    return exported_tables"
      ],
      "metadata": {
        "id": "H8l0_FiWliaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VLM PROCESSING - ANALISI IMMAGINI CON VISION LANGUAGE MODEL\n",
        "\n",
        "def add_vlm_descriptions_to_images(exported_images: List[Dict], doc_id: str) -> List[Dict]:\n",
        "\n",
        "    # Genera descrizioni testuali delle immagini usando Vision Language Model.\n",
        "    # Processo:\n",
        "    # 1) Carica modello SmolVLM (256M parametri, ottimizzato per Colab)\n",
        "    # 2) Per ogni immagine, genera descrizione dettagliata\n",
        "    # 3) Salva descrizione sia nel JSON che in file .txt separato\n",
        "\n",
        "    # IMPORTANTE per Vector Database Ogni immagine ha un file .txt con la descrizione VLM, Permette indicizzazione semantica delle immagini\n",
        "\n",
        "    # Args: exported_images Lista metadati immagini, doc_id Identificativo documento\n",
        "    # Returns: Lista aggiornata con descrizioni VLM\n",
        "\n",
        "    if not exported_images or not INAILConfig.VLM_ENABLED:\n",
        "        return exported_images\n",
        "\n",
        "    try:\n",
        "        from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "        from PIL import Image\n",
        "        import torch\n",
        "\n",
        "        print(f\"\\n[VLM] Caricamento modello {INAILConfig.VLM_MODEL}...\")\n",
        "\n",
        "        # Carica processore e modello\n",
        "        processor = AutoProcessor.from_pretrained(\n",
        "            INAILConfig.VLM_MODEL,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        model = AutoModelForVision2Seq.from_pretrained(\n",
        "            INAILConfig.VLM_MODEL,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Processa ogni immagine\n",
        "        for idx, img_data in enumerate(exported_images, 1):\n",
        "            try:\n",
        "                print(f\"  [{idx}/{len(exported_images)}] Analisi: {img_data['filename']}\")\n",
        "\n",
        "                # Carica immagine\n",
        "                image = Image.open(img_data['path']).convert('RGB')\n",
        "\n",
        "                # Prepara prompt per VLM\n",
        "                messages = [{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"image\"},\n",
        "                        {\"type\": \"text\", \"text\": INAILConfig.VLM_PROMPT}\n",
        "                    ]\n",
        "                }]\n",
        "\n",
        "                # Genera descrizione\n",
        "                prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "                inputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(**inputs, max_new_tokens=300, do_sample=False)\n",
        "\n",
        "                # Estrai descrizione dall'output\n",
        "                full_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "                description = full_output.split(\"Assistant:\")[-1].strip() \\\n",
        "                             if \"Assistant:\" in full_output else full_output.strip()\n",
        "\n",
        "                # Salva nel dizionario\n",
        "                img_data['vlm_description'] = description\n",
        "\n",
        "                # SALVA FILE .TXT con descrizione VLM\n",
        "                # Questo Ã¨ CRUCIALE per il vector database!\n",
        "                img_path = Path(img_data['path'])\n",
        "                txt_path = img_path.with_suffix('.txt')\n",
        "\n",
        "                with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "                    f.write(\"=\"*80 + \"\\n\")\n",
        "                    f.write(f\"VLM DESCRIPTION - {img_data['filename']}\\n\")\n",
        "                    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "                    f.write(\"IMAGE METADATA:\\n\")\n",
        "                    f.write(f\"  - Filename: {img_data['filename']}\\n\")\n",
        "                    f.write(f\"  - Caption: {img_data.get('caption', 'N/A')}\\n\")\n",
        "                    f.write(f\"  - Position: {img_data.get('position', 'N/A')}\\n\")\n",
        "                    f.write(f\"  - Model: {INAILConfig.VLM_MODEL}\\n\")\n",
        "                    f.write(f\"  - Timestamp: {datetime.now().isoformat()}\\n\\n\")\n",
        "\n",
        "                    f.write(\"VLM ANALYSIS:\\n\")\n",
        "                    f.write(\"-\"*80 + \"\\n\")\n",
        "                    f.write(description)\n",
        "                    f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "                img_data['vlm_description_file'] = str(txt_path)\n",
        "                print(f\"    âœ“ Descrizione: {description[:60]}...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    âœ— Errore: {e}\")\n",
        "                img_data['vlm_description'] = None\n",
        "                img_data['vlm_description_file'] = None\n",
        "\n",
        "        # Cleanup memoria\n",
        "        del model, processor\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        success = sum(1 for img in exported_images if img.get('vlm_description'))\n",
        "        print(f\"[VLM] Completato: {success}/{len(exported_images)} descrizioni generate\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] VLM fallito: {e}\")\n",
        "\n",
        "    return exported_images"
      ],
      "metadata": {
        "id": "tyAs8m21l47L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN SCRAPING - PROCESSAMENTO SINGOLA PUBBLICAZIONE\n",
        "\n",
        "def scrape_inail_publication(driver, publication_url: str, converter,\n",
        "                             save_pdf: bool = True,\n",
        "                             enable_vlm: Optional[bool] = None) -> Dict[str, Any]:\n",
        "\n",
        "    # Scrapa una singola pubblicazione INAIL completa.\n",
        "\n",
        "    # Pipeline completa:\n",
        "    # 1) Estrae metadati dalla pagina web (titolo, data, abstract)\n",
        "    # 2) Scarica PDF\n",
        "    # 3) Processa PDF con Docling (testo, tabelle, immagini)\n",
        "    # 4) Analizza struttura documento\n",
        "    # 5) Genera descrizioni VLM per immagini\n",
        "    # 6) Salva tutto in JSON strutturato\n",
        "\n",
        "    # Args: driver Selenium WebDriver, publication_url URL della pubblicazione, converter Docling converter, save_pdf Se True, mantiene PDF scaricato, enable_vlm Abilita/disabilita VLM (None = usa default config)\n",
        "    # Returns: Dati completi pubblicazione processata\n",
        "\n",
        "    use_vlm = enable_vlm if enable_vlm is not None else INAILConfig.VLM_ENABLED\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"SCRAPING PUBBLICAZIONE\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    scraping_metadata = {\n",
        "        'url': publication_url,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'scraper_version': '1.0_THESIS',\n",
        "        'docling_used': True,\n",
        "        'vlm_enabled': use_vlm\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # STEP 1: Estrai metadati web\n",
        "        driver.get(publication_url)\n",
        "        time.sleep(3)\n",
        "\n",
        "        page = BeautifulSoup(driver.page_source, \"lxml\")\n",
        "\n",
        "        # Titolo\n",
        "        title_elem = page.find(\"h2\", class_=\"h1\")\n",
        "        title = title_elem.get_text(strip=True) if title_elem else \"N/A\"\n",
        "\n",
        "        # Abstract/Descrizione\n",
        "        descr_blocks = page.find_all(\"p\", class_=\"text-20\")\n",
        "        abstract = descr_blocks[1].get_text(strip=True) if len(descr_blocks) > 1 else \"\"\n",
        "\n",
        "        # Data pubblicazione\n",
        "        data_elem = page.find(\"strong\", class_=\"js-date-value\")\n",
        "        data_pub = data_elem.get_text(strip=True).split(\", \")[0] if data_elem else \"N/A\"\n",
        "\n",
        "        # Link PDF\n",
        "        link_pdf = page.find(\"ul\", class_=\"list-download\")\n",
        "        pdf_url = None\n",
        "        if link_pdf:\n",
        "            a_tag = link_pdf.find(\"a\", href=True)\n",
        "            if a_tag:\n",
        "                pdf_url = urljoin(\"https://www.inail.it\", a_tag[\"href\"])\n",
        "\n",
        "        print(f\"[+] Titolo: {title[:60]}...\")\n",
        "        print(f\"[+] Data: {data_pub}\")\n",
        "        print(f\"[+] PDF: {'Disponibile' if pdf_url else 'Non trovato'}\")\n",
        "\n",
        "        # STEP 2: Processa PDF\n",
        "        pdf_data = None\n",
        "        pdf_local_path = None\n",
        "\n",
        "        if pdf_url:\n",
        "            # Genera nome file sicuro\n",
        "            safe_filename = re.sub(r'[^\\w\\-_.]', '_', title[:50]) + '.pdf'\n",
        "            pdf_local_path = INAILConfig.PDF_DIR / safe_filename\n",
        "            doc_id = safe_filename.replace('.pdf', '')\n",
        "\n",
        "            print(f\"\\n[INFO] Download e processing PDF...\")\n",
        "\n",
        "            if download_pdf_inail(pdf_url, pdf_local_path):\n",
        "                # Converti PDF con Docling\n",
        "                result = converter.convert(str(pdf_local_path))\n",
        "                doc = result.document\n",
        "\n",
        "                # Esporta formati\n",
        "                markdown_text = doc.export_to_markdown()\n",
        "                plain_text = doc.export_to_text()\n",
        "\n",
        "                # Analizza struttura\n",
        "                structure_info = analyze_document_structure(doc)\n",
        "\n",
        "                # Estrai immagini\n",
        "                exported_images = export_images_from_document(doc, doc_id)\n",
        "\n",
        "                # VLM su immagini\n",
        "                if use_vlm and exported_images:\n",
        "                    exported_images = add_vlm_descriptions_to_images(exported_images, doc_id)\n",
        "\n",
        "                    # Aggiorna figure info con descrizioni VLM\n",
        "                    for idx, fig_info in enumerate(structure_info.get('figures', [])):\n",
        "                        if idx < len(exported_images):\n",
        "                            vlm_desc = exported_images[idx].get('vlm_description')\n",
        "                            if vlm_desc:\n",
        "                                fig_info['vlm_description'] = vlm_desc\n",
        "\n",
        "                # Esporta tabelle\n",
        "                exported_tables = export_tables_to_files(structure_info['tables'], doc_id)\n",
        "\n",
        "                # Statistiche\n",
        "                num_pages = len(doc.pages) if hasattr(doc, 'pages') else 'N/A'\n",
        "                print(f\"\\n[OK] Processamento completato:\")\n",
        "                print(f\"  - Pagine: {num_pages}\")\n",
        "                print(f\"  - Tabelle: {structure_info['num_tables']}\")\n",
        "                print(f\"  - Figure: {structure_info['num_figures']}\")\n",
        "                print(f\"  - Headings: {structure_info['num_headings']}\")\n",
        "\n",
        "                # Prepara dati per JSON\n",
        "                pdf_data = {\n",
        "                    'markdown_content': markdown_text,\n",
        "                    'plain_text': plain_text,\n",
        "                    'num_pages': num_pages,\n",
        "                    'num_tables': structure_info['num_tables'],\n",
        "                    'num_figures': structure_info['num_figures'],\n",
        "                    'num_headings': structure_info['num_headings'],\n",
        "                    'headings': structure_info['headings'],\n",
        "                    'tables': structure_info['tables'],\n",
        "                    'figures': structure_info['figures'],\n",
        "                    'exported_images': exported_images,\n",
        "                    'exported_tables': exported_tables\n",
        "                }\n",
        "\n",
        "                # Rimuovi PDF se non richiesto\n",
        "                if not save_pdf:\n",
        "                    pdf_local_path.unlink()\n",
        "                    pdf_local_path = None\n",
        "\n",
        "        # STEP 3: Ritorna risultato\n",
        "        return {\n",
        "            'scraping_metadata': scraping_metadata,\n",
        "            'web_metadata': {\n",
        "                'title': title,\n",
        "                'abstract': abstract,\n",
        "                'data_pubblicazione': data_pub,\n",
        "                'pdf_url': pdf_url,\n",
        "                'pdf_local_path': str(pdf_local_path) if pdf_local_path else None\n",
        "            },\n",
        "            'document_content': pdf_data,\n",
        "            'status': 'success',\n",
        "            'has_pdf': pdf_url is not None,\n",
        "            'pdf_processed': pdf_data is not None\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Scraping fallito: {e}\")\n",
        "        return {\n",
        "            'scraping_metadata': scraping_metadata,\n",
        "            'status': 'error',\n",
        "            'error_message': str(e)\n",
        "        }\n",
        "\n",
        "\n",
        "def save_to_json(data: Dict[str, Any], output_name: Optional[str] = None) -> Path:\n",
        "\n",
        "    # Salva dati pubblicazione in file JSON\n",
        "    # Args: Dati da salvare, output_name Nome file (opzionale, auto-generato se None)\n",
        "    # Returns: Path del file JSON creato\n",
        "\n",
        "    if output_name is None:\n",
        "        title = data.get('web_metadata', {}).get('title', 'unknown')\n",
        "        safe_title = re.sub(r'[^\\w\\-_.]', '_', title[:50])\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        output_name = f\"{safe_title}_{timestamp}.json\"\n",
        "\n",
        "    output_path = INAILConfig.JSON_DIR / output_name\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "LiMHIEgWmU-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCRAPING INCREMENTALE - SKIP DOCUMENTI GIÃ€ PROCESSATI\n",
        "\n",
        "def load_already_scraped_urls() -> set:\n",
        "\n",
        "    # Carica URL di tutte le pubblicazioni giÃ  processate.\n",
        "    # Cerca in File JSON locali e Backup su Drive (se disponibile)\n",
        "    # Questo permette scraping incrementale: nuove esecuzioni skippano automaticamente documenti giÃ  scaricati\n",
        "    # Returns: set Set di URL giÃ  processati\n",
        "\n",
        "    scraped_urls = set()\n",
        "\n",
        "    # Directory dove cercare JSON\n",
        "    search_dirs = [INAILConfig.JSON_DIR]\n",
        "\n",
        "    if INAILConfig.DRIVE_BACKUP_DIR.exists():\n",
        "        drive_json_dir = INAILConfig.DRIVE_BACKUP_DIR / 'json'\n",
        "        if drive_json_dir.exists():\n",
        "            search_dirs.append(drive_json_dir)\n",
        "\n",
        "    # Scansiona tutti i JSON\n",
        "    for json_dir in search_dirs:\n",
        "        if not json_dir.exists():\n",
        "            continue\n",
        "\n",
        "        json_files = list(json_dir.glob(\"*.json\"))\n",
        "        json_files = [f for f in json_files if not f.name.startswith('vector_db_manifest')]\n",
        "\n",
        "        for json_file in json_files:\n",
        "            try:\n",
        "                with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                # Gestisci sia file singoli che batch\n",
        "                if isinstance(data, list):\n",
        "                    docs = data\n",
        "                else:\n",
        "                    docs = [data]\n",
        "\n",
        "                # Estrai URL\n",
        "                for doc in docs:\n",
        "                    url = doc.get('scraping_metadata', {}).get('url')\n",
        "                    if url:\n",
        "                        scraped_urls.add(url)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    return scraped_urls\n",
        "\n",
        "\n",
        "def batch_scrape_inail_publications_incremental(\n",
        "    driver, converter,\n",
        "    query: str = None,\n",
        "    max_pages: int = 5,\n",
        "    max_documents: Optional[int] = None,\n",
        "    enable_vlm: bool = False,\n",
        "    save_pdfs: bool = True,\n",
        "    delay_between_docs: int = 4,\n",
        "    auto_backup_every: int = 5\n",
        ") -> List[Dict[str, Any]]:\n",
        "\n",
        "    # Scraping batch incrementale di pubblicazioni INAIL\n",
        "    # Caratteristiche chiave: Skip automatico documenti giÃ  processati, Backup automatico ogni N documenti, Gestione interruzioni, Statistiche in tempo reale\n",
        "\n",
        "    # Args:\n",
        "    #    driver: Selenium WebDriver\n",
        "    #    converter: Docling converter\n",
        "    #    query: Query di ricerca (None = tutte)\n",
        "    #    max_pages: Numero massimo pagine da esplorare\n",
        "    #    max_documents: Limite documenti da processare (None = tutti)\n",
        "    #    enable_vlm: Abilita analisi VLM immagini\n",
        "    #    save_pdfs: Mantieni PDF scaricati\n",
        "    #    delay_between_docs: Secondi tra documenti\n",
        "    #   auto_backup_every: Backup ogni N documenti processati\n",
        "\n",
        "    # Returns: Lista documenti processati con successo\n",
        "\n",
        "\n",
        "    # STEP 1: Carica documenti giÃ  fatti\n",
        "    already_scraped = load_already_scraped_urls()\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"SCRAPING INCREMENTALE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"ðŸ“Š Documenti giÃ  processati: {len(already_scraped)}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # STEP 2: Estrai link pubblicazioni\n",
        "    publication_links = get_inail_publication_links(driver, query=query, max_pages=max_pages)\n",
        "\n",
        "    if not publication_links:\n",
        "        print(\"[WARNING] Nessuna pubblicazione trovata\")\n",
        "        return []\n",
        "\n",
        "    # STEP 3: Filtra solo documenti nuovi\n",
        "    new_links = [pub for pub in publication_links if pub['url'] not in already_scraped]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FILTRO DOCUMENTI\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"ðŸ“„ Totali trovati: {len(publication_links)}\")\n",
        "    print(f\"â­ï¸  GiÃ  processati: {len(publication_links) - len(new_links)}\")\n",
        "    print(f\"ðŸ†• Nuovi da processare: {len(new_links)}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    if not new_links:\n",
        "        print(\"âœ… Tutti i documenti sono giÃ  stati processati!\")\n",
        "        return []\n",
        "\n",
        "    # Limita numero documenti se richiesto\n",
        "    if max_documents:\n",
        "        new_links = new_links[:max_documents]\n",
        "        print(f\"[INFO] Limitato a {max_documents} nuovi documenti\\n\")\n",
        "\n",
        "    # STEP 4: Scraping documenti nuovi\n",
        "    results = []\n",
        "    failed = []\n",
        "\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"SCRAPING {len(new_links)} NUOVI DOCUMENTI\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    for idx, pub in enumerate(new_links, 1):\n",
        "        print(f\"\\n[{idx}/{len(new_links)}] {pub['titolo'][:50]}...\")\n",
        "\n",
        "        try:\n",
        "            # Scrapa pubblicazione\n",
        "            result = scrape_inail_publication(\n",
        "                driver=driver,\n",
        "                publication_url=pub['url'],\n",
        "                converter=converter,\n",
        "                save_pdf=save_pdfs,\n",
        "                enable_vlm=enable_vlm\n",
        "            )\n",
        "\n",
        "            if result['status'] == 'success':\n",
        "                results.append(result)\n",
        "                save_to_json(result)\n",
        "                print(f\"  âœ… Successo\")\n",
        "\n",
        "                # Backup automatico periodico\n",
        "                if idx % auto_backup_every == 0:\n",
        "                    print(f\"\\n  â˜ï¸  Auto-backup ({idx}/{len(new_links)})...\")\n",
        "                    backup_to_drive()\n",
        "            else:\n",
        "                failed.append({\n",
        "                    'url': pub['url'],\n",
        "                    'title': pub['titolo'],\n",
        "                    'error': result.get('error_message', 'Unknown')\n",
        "                })\n",
        "                print(f\"  âŒ Fallito\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(f\"\\nâš ï¸ INTERRUZIONE MANUALE\")\n",
        "            print(f\"ðŸ’¾ Backup dati salvati...\")\n",
        "            backup_to_drive()\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Eccezione: {str(e)[:80]}\")\n",
        "            failed.append({\n",
        "                'url': pub['url'],\n",
        "                'title': pub['titolo'],\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "        # Pausa tra documenti\n",
        "        if idx < len(new_links):\n",
        "            delay = random.uniform(delay_between_docs, delay_between_docs + 2)\n",
        "            time.sleep(delay)\n",
        "\n",
        "    # STEP 5: Backup finale\n",
        "    if results:\n",
        "        print(f\"\\nâ˜ï¸  Backup finale su Drive...\")\n",
        "        backup_to_drive()\n",
        "\n",
        "    # STEP 6: Statistiche finali\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"SCRAPING COMPLETATO\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"âœ… Nuovi documenti: {len(results)}\")\n",
        "    print(f\"âŒ Falliti: {len(failed)}\")\n",
        "    print(f\"ðŸ“Š Totale documenti: {len(already_scraped) + len(results)}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "bqTNruQCn45w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VECTOR DATABASE - MANIFEST GENERATION\n",
        "\n",
        "def create_vector_db_manifest(json_dir: Path = None) -> Path:\n",
        "\n",
        "    # Crea manifest JSON aggregato per indicizzazione in vector database\n",
        "    # Il manifest organizza TUTTI i contenuti testuali per l'embedding: Testi markdown completi, Tabelle con metadati (colonne, caption), Immagini con descrizioni VLM\n",
        "\n",
        "    # Struttura manifest:\n",
        "    # {\n",
        "    #  \"documents\": [\n",
        "    #    {\n",
        "    #      \"document_id\": \"...\",\n",
        "    #      \"title\": \"...\",\n",
        "    #      \"indexable_content\": [\n",
        "    #        {\"type\": \"markdown_text\", \"content\": \"...\", \"metadata\": {...}},\n",
        "    #        {\"type\": \"table\", \"content\": \"...\", \"metadata\": {...}},\n",
        "    #        {\"type\": \"image_vlm\", \"content\": \"...\", \"metadata\": {...}}\n",
        "    #      ]\n",
        "    #   }\n",
        "    #  ]\n",
        "    # }\n",
        "\n",
        "    # Args: json_dir Directory JSON (default: INAILConfig.JSON_DIR)\n",
        "    # Returns: Path del manifest creato\n",
        "\n",
        "    if json_dir is None:\n",
        "        json_dir = INAILConfig.JSON_DIR\n",
        "\n",
        "    manifest = {\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'purpose': 'Vector database indexing manifest',\n",
        "        'version': '1.0',\n",
        "        'total_documents': 0,\n",
        "        'documents': []\n",
        "    }\n",
        "\n",
        "    json_files = list(json_dir.glob(\"*.json\"))\n",
        "    json_files = [f for f in json_files if not f.name.startswith('vector_db_manifest')]\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"CREAZIONE MANIFEST PER VECTOR DB\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Gestisci sia file singoli che batch\n",
        "            if isinstance(data, list):\n",
        "                docs_to_process = data\n",
        "            else:\n",
        "                docs_to_process = [data]\n",
        "\n",
        "            for doc in docs_to_process:\n",
        "                if doc.get('status') != 'success':\n",
        "                    continue\n",
        "\n",
        "                doc_content = doc.get('document_content')\n",
        "                if not doc_content:\n",
        "                    continue\n",
        "\n",
        "                web_meta = doc.get('web_metadata', {})\n",
        "                doc_id = re.sub(r'[^\\w\\-_.]', '_', web_meta.get('title', 'unknown')[:50])\n",
        "\n",
        "                # Entry per vector DB\n",
        "                vector_entry = {\n",
        "                    'document_id': doc_id,\n",
        "                    'title': web_meta.get('title', 'N/A'),\n",
        "                    'publication_date': web_meta.get('data_pubblicazione', 'N/A'),\n",
        "                    'source_url': doc.get('scraping_metadata', {}).get('url'),\n",
        "                    'pdf_url': web_meta.get('pdf_url'),\n",
        "                    'content_types': [],\n",
        "                    'indexable_content': []\n",
        "                }\n",
        "\n",
        "                # 1) TESTO MARKDOWN\n",
        "                if doc_content.get('markdown_content'):\n",
        "                    vector_entry['content_types'].append('markdown_text')\n",
        "                    vector_entry['indexable_content'].append({\n",
        "                        'type': 'markdown_text',\n",
        "                        'content': doc_content['markdown_content'],\n",
        "                        'metadata': {\n",
        "                            'num_pages': doc_content.get('num_pages', 'N/A'),\n",
        "                            'num_headings': doc_content.get('num_headings', 0)\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "                # 2) TABELLE con metadati\n",
        "                for table_info in doc_content.get('tables', []):\n",
        "                    vector_entry['content_types'].append('table')\n",
        "\n",
        "                    # Testo arricchito: caption + colonne + contenuto\n",
        "                    table_text = f\"TABELLA: {table_info.get('caption', 'Senza titolo')}\\n\\n\"\n",
        "\n",
        "                    if table_info.get('potential_columns'):\n",
        "                        table_text += \"COLONNE: \" + \", \".join(table_info['potential_columns']) + \"\\n\\n\"\n",
        "\n",
        "                    table_text += \"CONTENUTO:\\n\" + table_info.get('text_content', '')\n",
        "\n",
        "                    vector_entry['indexable_content'].append({\n",
        "                        'type': 'table',\n",
        "                        'content': table_text,\n",
        "                        'metadata': {\n",
        "                            'table_id': table_info.get('table_id'),\n",
        "                            'caption': table_info.get('caption'),\n",
        "                            'num_rows': table_info.get('num_rows'),\n",
        "                            'num_columns': table_info.get('num_columns'),\n",
        "                            'columns': table_info.get('potential_columns', []),\n",
        "                            'position': table_info.get('position')\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "                # 3) IMMAGINI con descrizioni VLM\n",
        "                for img_info in doc_content.get('exported_images', []):\n",
        "                    if img_info.get('vlm_description'):\n",
        "                        vector_entry['content_types'].append('image_vlm')\n",
        "\n",
        "                        # Testo combinato: caption + descrizione VLM\n",
        "                        img_text = f\"IMMAGINE: {img_info.get('filename')}\\n\"\n",
        "                        img_text += f\"CAPTION: {img_info.get('caption', 'N/A')}\\n\\n\"\n",
        "                        img_text += f\"DESCRIZIONE VISIVA:\\n{img_info['vlm_description']}\"\n",
        "\n",
        "                        # Path file .txt con descrizione\n",
        "                        img_path = Path(img_info.get('path', ''))\n",
        "                        txt_path = img_path.with_suffix('.txt')\n",
        "                        vlm_file = str(txt_path) if txt_path.exists() else None\n",
        "\n",
        "                        vector_entry['indexable_content'].append({\n",
        "                            'type': 'image_vlm',\n",
        "                            'content': img_text,\n",
        "                            'metadata': {\n",
        "                                'filename': img_info.get('filename'),\n",
        "                                'caption': img_info.get('caption'),\n",
        "                                'vlm_description_file': vlm_file,\n",
        "                                'image_path': img_info.get('path'),\n",
        "                                'position': img_info.get('position')\n",
        "                            }\n",
        "                        })\n",
        "\n",
        "                # Aggiungi al manifest se ha contenuto\n",
        "                if vector_entry['indexable_content']:\n",
        "                    manifest['documents'].append(vector_entry)\n",
        "                    manifest['total_documents'] += 1\n",
        "                    print(f\"  âœ“ {doc_id}: {len(vector_entry['indexable_content'])} chunks\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âœ— Errore {json_file.name}: {e}\")\n",
        "\n",
        "    # Salva manifest\n",
        "    manifest_path = json_dir / f\"vector_db_manifest_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "    with open(manifest_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Statistiche\n",
        "    total_chunks = sum(len(doc['indexable_content']) for doc in manifest['documents'])\n",
        "    total_tables = sum(1 for doc in manifest['documents']\n",
        "                      for chunk in doc['indexable_content']\n",
        "                      if chunk['type'] == 'table')\n",
        "    total_images = sum(1 for doc in manifest['documents']\n",
        "                      for chunk in doc['indexable_content']\n",
        "                      if chunk['type'] == 'image_vlm')\n",
        "    total_markdown = sum(1 for doc in manifest['documents']\n",
        "                        for chunk in doc['indexable_content']\n",
        "                        if chunk['type'] == 'markdown_text')\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MANIFEST CREATO\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"ðŸ“„ File: {manifest_path.name}\")\n",
        "    print(f\"ðŸ“Š Documenti: {manifest['total_documents']}\")\n",
        "    print(f\"ðŸ“¦ Chunks totali: {total_chunks}\")\n",
        "    print(f\"   - Testi markdown: {total_markdown}\")\n",
        "    print(f\"   - Tabelle: {total_tables}\")\n",
        "    print(f\"   - Immagini VLM: {total_images}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return manifest_path"
      ],
      "metadata": {
        "id": "tSn-sklhoZm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN - INTERFACCIA UTENTE\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Funzione principale con menu interattivo.\n",
        "    # Gestisce: Setup iniziale (directory, Drive, Docling), Ripristino automatico da backup, Menu scelta modalitÃ , Scraping incrementale, Generazione manifest\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INAIL SCRAPER - VERSIONE FINALE PER TESI\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # STEP 1) Setup directories\n",
        "    INAILConfig.setup_directories()\n",
        "\n",
        "    # STEP 2) Google Drive + ripristino\n",
        "    print(\"\\n[STEP 1] Setup Google Drive...\")\n",
        "    if mount_google_drive():\n",
        "        print(\"\\n[STEP 2] Ripristino dati da backup Drive...\")\n",
        "        restore_from_drive()\n",
        "\n",
        "    # STEP 3) Mostra statistiche documenti giÃ  processati\n",
        "    already_scraped = load_already_scraped_urls()\n",
        "    print(f\"\\n[INFO] ðŸ“Š Documenti giÃ  processati: {len(already_scraped)}\")\n",
        "\n",
        "    # STEP 4) Inizializza Docling\n",
        "    print(\"\\n[STEP 3] Inizializzazione Docling...\")\n",
        "    converter = initialize_docling_converter()\n",
        "\n",
        "    # STEP 5) Menu scelta modalitÃ \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODALITÃ€ DISPONIBILI:\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"1. ðŸš€ Scraping incrementale (skippa giÃ  processati)\")\n",
        "    print(\"2. ðŸ“Š Verifica stato (statistiche documenti)\")\n",
        "    print(\"3. ðŸ“‹ Crea manifest per Vector DB\")\n",
        "    print(\"4. â˜ï¸  Backup manuale su Drive\")\n",
        "    print(\"5. ðŸ”„ Ripristino da Drive\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    choice = input(\"Scegli modalitÃ  (1-5, default 1): \").strip() or \"1\"\n",
        "\n",
        "    # MODALITÃ€ 2: Verifica Stato\n",
        "    if choice == \"2\":\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"STATO ATTUALE SISTEMA\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"ðŸ“Š Documenti processati: {len(already_scraped)}\")\n",
        "        print(f\"ðŸ“ Path locale: {INAILConfig.OUTPUT_DIR}\")\n",
        "        print(f\"â˜ï¸  Path Drive: {INAILConfig.DRIVE_BACKUP_DIR}\")\n",
        "\n",
        "        # Conta file per tipo\n",
        "        if INAILConfig.OUTPUT_DIR.exists():\n",
        "            pdfs = len(list(INAILConfig.PDF_DIR.glob('*.pdf'))) if INAILConfig.PDF_DIR.exists() else 0\n",
        "            jsons = len(list(INAILConfig.JSON_DIR.glob('*.json'))) if INAILConfig.JSON_DIR.exists() else 0\n",
        "            images = sum(1 for _ in INAILConfig.IMAGES_DIR.rglob('*.png')) if INAILConfig.IMAGES_DIR.exists() else 0\n",
        "            tables = sum(1 for _ in INAILConfig.TABLES_DIR.rglob('*.txt')) if INAILConfig.TABLES_DIR.exists() else 0\n",
        "\n",
        "            print(f\"\\nðŸ“¦ File locali:\")\n",
        "            print(f\"   - PDF: {pdfs}\")\n",
        "            print(f\"   - JSON: {jsons}\")\n",
        "            print(f\"   - Immagini: {images}\")\n",
        "            print(f\"   - Tabelle: {tables}\")\n",
        "\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        return None\n",
        "\n",
        "    # MODALITÃ€ 3: Crea Manifest\n",
        "    elif choice == \"3\":\n",
        "        manifest_path = create_vector_db_manifest()\n",
        "        print(f\"âœ… Manifest creato: {manifest_path}\")\n",
        "\n",
        "        # Backup manifest su Drive\n",
        "        backup_choice = input(\"\\nBackup manifest su Drive? (y/n): \").strip().lower()\n",
        "        if backup_choice == 'y':\n",
        "            backup_to_drive()\n",
        "\n",
        "        return None\n",
        "\n",
        "    # MODALITÃ€ 4: Backup Manuale\n",
        "    elif choice == \"4\":\n",
        "        backup_to_drive()\n",
        "        return None\n",
        "\n",
        "    # MODALITÃ€ 5: Ripristino\n",
        "    elif choice == \"5\":\n",
        "        restore_from_drive()\n",
        "        return None\n",
        "\n",
        "    # MODALITÃ€ 1: Scraping Incrementale\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CONFIGURAZIONE SCRAPING INCREMENTALE\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Parametri ricerca\n",
        "    query = input(\"ðŸ” Query ricerca (Enter = tutte le pubblicazioni): \").strip() or None\n",
        "\n",
        "    try:\n",
        "        max_pages = int(input(\"ðŸ“„ Numero massimo pagine da esplorare (default 5): \").strip() or \"5\")\n",
        "    except:\n",
        "        max_pages = 5\n",
        "\n",
        "    max_docs_input = input(\"ðŸ“Š Limite documenti da processare (Enter = tutti): \").strip()\n",
        "    max_docs = int(max_docs_input) if max_docs_input else None\n",
        "\n",
        "    vlm_input = input(\"ðŸ¤– Abilitare analisi VLM immagini? (y/n, default y): \").strip().lower()\n",
        "    enable_vlm = (vlm_input != 'n')\n",
        "\n",
        "    # Riepilogo configurazione\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"RIEPILOGO CONFIGURAZIONE:\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"ðŸ” Query: {query if query else '(tutte le pubblicazioni)'}\")\n",
        "    print(f\"ðŸ“„ Pagine da esplorare: {max_pages}\")\n",
        "    print(f\"ðŸ“Š Limite documenti: {max_docs if max_docs else '(tutti i nuovi)'}\")\n",
        "    print(f\"ðŸ¤– Analisi VLM: {'âœ… Abilitata' if enable_vlm else 'âŒ Disabilitata'}\")\n",
        "    print(f\"â­ï¸  Documenti giÃ  processati: {len(already_scraped)} (verranno skippati)\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    confirm = input(\"â–¶ï¸  Avviare scraping? (y/n): \").strip().lower()\n",
        "    if confirm != 'y':\n",
        "        print(\"\\nâŒ Operazione annullata\")\n",
        "        return None\n",
        "\n",
        "    # Esegui scraping incrementale\n",
        "    results = batch_scrape_inail_publications_incremental(\n",
        "        driver=driver,\n",
        "        converter=converter,\n",
        "        query=query,\n",
        "        max_pages=max_pages,\n",
        "        max_documents=max_docs,\n",
        "        enable_vlm=enable_vlm,\n",
        "        save_pdfs=True,\n",
        "        delay_between_docs=4,\n",
        "        auto_backup_every=5\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "jVKzNDvSoe18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UTILITY FUNCTIONS - COMANDI RAPIDI\n",
        "\n",
        "def quick_backup():\n",
        "    # Comando rapido per backup su Drive\n",
        "    mount_google_drive()\n",
        "    backup_to_drive()\n",
        "\n",
        "def quick_restore():\n",
        "    # Comando rapido per ripristino da Drive\n",
        "    mount_google_drive()\n",
        "    restore_from_drive()\n",
        "\n",
        "def verify_vector_db_readiness():\n",
        "\n",
        "    # Verifica che tutti i file siano pronti per vector database\n",
        "    # Controlla Ogni immagine ha il suo file .txt con descrizione VLM, Tabelle estratte correttamente, Manifest generato\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"VERIFICA PREPARAZIONE VECTOR DB\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    issues = []\n",
        "\n",
        "    # Verifica immagini + descrizioni VLM\n",
        "    if INAILConfig.IMAGES_DIR.exists():\n",
        "        for doc_dir in INAILConfig.IMAGES_DIR.iterdir():\n",
        "            if doc_dir.is_dir():\n",
        "                png_files = list(doc_dir.glob(\"*.png\"))\n",
        "                txt_files = list(doc_dir.glob(\"*.txt\"))\n",
        "\n",
        "                print(f\"ðŸ“ {doc_dir.name}:\")\n",
        "                print(f\"   - Immagini: {len(png_files)}\")\n",
        "                print(f\"   - Descrizioni VLM: {len(txt_files)}\")\n",
        "\n",
        "                if len(png_files) != len(txt_files):\n",
        "                    issues.append(f\"{doc_dir.name}: {len(png_files)} immagini ma {len(txt_files)} descrizioni\")\n",
        "                    print(f\"   âš ï¸  MISMATCH!\")\n",
        "                else:\n",
        "                    print(f\"   âœ… OK\")\n",
        "\n",
        "    # Verifica tabelle\n",
        "    if INAILConfig.TABLES_DIR.exists():\n",
        "        total_tables = sum(1 for _ in INAILConfig.TABLES_DIR.rglob(\"*.txt\"))\n",
        "        print(f\"\\nðŸ“Š Tabelle estratte: {total_tables}\")\n",
        "\n",
        "    # Verifica JSON\n",
        "    json_files = list(INAILConfig.JSON_DIR.glob(\"*.json\")) if INAILConfig.JSON_DIR.exists() else []\n",
        "    print(f\"\\nðŸ“„ File JSON: {len(json_files)}\")\n",
        "\n",
        "    # Verifica manifest\n",
        "    manifest_files = list(INAILConfig.JSON_DIR.glob(\"vector_db_manifest*.json\")) if INAILConfig.JSON_DIR.exists() else []\n",
        "    if manifest_files:\n",
        "        print(f\"ðŸ“‹ Manifest Vector DB: âœ… {len(manifest_files)} file\")\n",
        "        latest = max(manifest_files, key=lambda p: p.stat().st_mtime)\n",
        "        print(f\"   Ultimo: {latest.name}\")\n",
        "    else:\n",
        "        print(f\"ðŸ“‹ Manifest Vector DB: âŒ Non trovato\")\n",
        "        print(f\"   â†’ Esegui: create_vector_db_manifest()\")\n",
        "\n",
        "    # Report finale\n",
        "    if issues:\n",
        "        print(f\"\\nâš ï¸  PROBLEMI RILEVATI:\")\n",
        "        for issue in issues:\n",
        "            print(f\"   - {issue}\")\n",
        "    else:\n",
        "        print(f\"\\nâœ… Tutto pronto per indicizzazione Vector Database!\")\n",
        "\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return len(issues) == 0"
      ],
      "metadata": {
        "id": "0q9qfwb1o1U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESECUZIONE SCRIPT\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Entry point dello script.\n",
        "    # Esegue 1) Main() per setup e scraping 2) Report finale con statistiche 3) Suggerimenti prossimi passi\n",
        "\n",
        "    # Esegui main\n",
        "    results = main()\n",
        "\n",
        "    # Report finale\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"REPORT FINALE ESECUZIONE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if results:\n",
        "        print(f\"âœ… Nuovi documenti processati: {len(results)}\")\n",
        "\n",
        "        # Statistiche dettagliate\n",
        "        total_images = sum(len(r.get('document_content', {}).get('exported_images', []))\n",
        "                          for r in results)\n",
        "        total_tables = sum(r.get('document_content', {}).get('num_tables', 0)\n",
        "                          for r in results)\n",
        "\n",
        "        print(f\"ðŸ“Š Statistiche sessione:\")\n",
        "        print(f\"   - Immagini estratte: {total_images}\")\n",
        "        print(f\"   - Tabelle estratte: {total_tables}\")\n",
        "\n",
        "    # Statistiche totali\n",
        "    total_docs = len(load_already_scraped_urls())\n",
        "    print(f\"\\nðŸ“Š Totale documenti nel dataset: {total_docs}\")\n",
        "    print(f\"ðŸ“ Output locale: {INAILConfig.OUTPUT_DIR}\")\n",
        "    print(f\"â˜ï¸  Backup Drive: {INAILConfig.DRIVE_BACKUP_DIR}\")\n",
        "\n",
        "    # Suggerimenti prossimi passi\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"PROSSIMI PASSI SUGGERITI:\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(\"1. Verifica preparazione Vector DB:\")\n",
        "    print(\"   â†’ verify_vector_db_readiness()\")\n",
        "    print(\"\\n2. Genera manifest per indicizzazione:\")\n",
        "    print(\"   â†’ create_vector_db_manifest()\")\n",
        "    print(\"\\n3. Backup finale su Drive:\")\n",
        "    print(\"   â†’ quick_backup()\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    print(\"âœ… Script completato con successo!\")"
      ],
      "metadata": {
        "id": "xGbdQ6zlpS_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXWXW6VyfzIK"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DOCUMENTAZIONE FINALE\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "================================================================================\n",
        "GUIDA UTILIZZO PER TESI\n",
        "================================================================================\n",
        "\n",
        "SETUP INIZIALE:\n",
        "1. Caricare questo script in Google Colab\n",
        "2. Eseguire tutte le celle\n",
        "3. Lo script monterÃ  automaticamente Google Drive\n",
        "4. I dati verranno salvati in: /content/drive/MyDrive/INAIL_Thesis_Data\n",
        "\n",
        "MODALITÃ€ SCRAPING:\n",
        "- Scraping incrementale: Skippa automaticamente documenti giÃ  processati\n",
        "- VLM opzionale: Genera descrizioni testuali delle immagini\n",
        "- Backup automatico: Salvataggio su Drive ogni 5 documenti\n",
        "\n",
        "OUTPUT GENERATO:\n",
        "1. PDF originali (opzionale)\n",
        "2. JSON con metadati completi\n",
        "3. Immagini estratte + descrizioni VLM (.txt)\n",
        "4. Tabelle estratte (.txt)\n",
        "5. Manifest per Vector Database\n",
        "\n",
        "STRUTTURA DATI PER VECTOR DB:\n",
        "vector_db_manifest_YYYYMMDD_HHMMSS.json\n",
        "â”œâ”€â”€ documents[]\n",
        "    â”œâ”€â”€ document_id\n",
        "    â”œâ”€â”€ title\n",
        "    â”œâ”€â”€ indexable_content[]\n",
        "        â”œâ”€â”€ type: \"markdown_text\" | \"table\" | \"image_vlm\"\n",
        "        â”œâ”€â”€ content: testo completo\n",
        "        â””â”€â”€ metadata: metadati specifici\n",
        "\n",
        "EMBEDDING E INDICIZZAZIONE:\n",
        "1. Caricare manifest JSON\n",
        "2. Per ogni chunk in indexable_content:\n",
        "   - Generare embedding (OpenAI/Cohere/etc)\n",
        "   - Inserire in Vector DB (ChromaDB/Pinecone/Weaviate)\n",
        "3. Conservare metadata per retrieval\n",
        "\n",
        "COMANDI UTILI:\n",
        "- quick_backup(): Backup rapido su Drive\n",
        "- quick_restore(): Ripristino da Drive\n",
        "- verify_vector_db_readiness(): Verifica dati per Vector DB\n",
        "- create_vector_db_manifest(): Genera manifest\n",
        "\n",
        "TROUBLESHOOTING:\n",
        "- Timeout Selenium: Lo script ricrea automaticamente il driver\n",
        "- Interruzione (Ctrl+C): Backup automatico dati salvati\n",
        "- Sessione Colab scaduta: Ri-eseguire, ripristino automatico da Drive\n",
        "\n",
        "RIFERIMENTI:\n",
        "- Docling: https://github.com/docling-project/docling\n",
        "- SmolVLM: https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct\n",
        "- INAIL Catalogo: https://www.inail.it/pubblicazioni\n",
        "\n",
        "================================================================================\n",
        "FINE SCRIPT\n",
        "================================================================================\n",
        "\"\"\""
      ]
    }
  ]
}